{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from pprint import pprint as pprint\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: 誰\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('誰'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: 誰\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    '''Takes in a character and returns an integer id number.'''\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    '''Takes the integer id number of a character and returns the character matching the string.'''\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('誰'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295486 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.99\n",
      "================================================================================\n",
      "z ikephskna omtmytewgrctoionopijm v  ler mscyrdtacngqvtejhgul bgtmzjlbgbnevltg  \n",
      "w ame ljqazfekrz b ahyepppuazk gn ebn ldlarfol sidgx dyassblerjsiqtderishvf japx\n",
      "gslorh qofjoa vnftfqjnrl b lrlujaiae nnq  ieecle tuiinqurfum witxa lrl fuix unzz\n",
      "bt ynxs eq qvtizlpezefanelnevhahrrlprwnlcmdndybvgprtzrnmqmntz nhg  filu kp a lul\n",
      "vxcxjirksdgnv epw i  qjkx i kr zahlggicuuaio tperm yhio gk ninkegeonpaqlatge er \n",
      "================================================================================\n",
      "Validation set perplexity: 20.28\n",
      "Average loss at step 100: 2.598224 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.10\n",
      "Validation set perplexity: 10.49\n",
      "Average loss at step 200: 2.265844 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.73\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 300: 2.103437 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 400: 2.004244 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 500: 1.936058 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 600: 1.910455 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 700: 1.858355 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.58\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 800: 1.820383 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 900: 1.827760 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1000: 1.825382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      " which the nimberba will kimon five dezered nerniar exvovern exbeet brece coull \n",
      "vernem hubparieg cherring yire b prowided to avruel prear of xurtriet the wrich \n",
      "rictry interdeary remeovibed fac onst whoulst interief heurruten icgual redemaud\n",
      "ther ievo where prhaes of eitr at bittre leads at dictly is the funal facts pres\n",
      "ken the easter in nine there impuarical oor ound four britwers areall conear in \n",
      "================================================================================\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1100: 1.774544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1200: 1.751554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1300: 1.729357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1400: 1.741331 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1500: 1.735386 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1600: 1.739570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1700: 1.710401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1800: 1.673812 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 1900: 1.645350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2000: 1.692467 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "vicata renets in tethnia aani the statter placting constingtieng bounding trone \n",
      "chiles were the postamitam sorbuted and addism maniatiby people pooned to langau\n",
      "may the notables postenwi s hoses mosts as medced and dairlesa the pacon films s\n",
      "quente is a lon linear bod xans mashs as be v s may ppsing becmutations and his \n",
      "k it not olatically seculation s asonghz onity as to housed on nicaral inalicam \n",
      "================================================================================\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2100: 1.684838 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2200: 1.679414 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2300: 1.640415 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2400: 1.655213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2500: 1.679484 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 2600: 1.654462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 2700: 1.655640 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 2800: 1.649801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 2900: 1.653851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3000: 1.652635 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "mence cal with him commoy this maws ups on old wa lise balks scrows scormo and l\n",
      "quile formality jamosing imen officied boince softics to of such communce houce \n",
      "fic is asually herdurblown besistinglal specing one three l succentern carthop s\n",
      "ters starge they it mahy resumpically game inflicted of sownd one and catcho or \n",
      "smenitabled injass a one one nine zero the pocitons in one fure which heige and \n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3100: 1.628975 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3200: 1.647392 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 3300: 1.637440 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 3400: 1.665547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3500: 1.656684 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3600: 1.664192 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 3700: 1.647138 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 3800: 1.643494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3900: 1.634934 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4000: 1.651364 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "quen wod contracuinal remainsuarallee or indepented the spart endayeas a then so\n",
      "mer one fives the systation mile volment wexual notcrihian viewed were juli in w\n",
      "thed his publites seatity mangokands the high whilipac destry iv from firetion r\n",
      "riagable f ears online is several appeanifists intex alband darily in first atla\n",
      " other caib to game it el other three zero zero the goun though interprekexbey e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4100: 1.632284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4200: 1.634863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 4300: 1.612798 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4400: 1.605944 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 4500: 1.613814 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4600: 1.614220 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4700: 1.626609 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4800: 1.629637 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 4900: 1.632805 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5000: 1.605189 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "s computine assitral vizor and thron which tomes passle uthide in his ben muses \n",
      "h enting sking five his pays one eight zero used their biscropwas are casitions \n",
      "ent of liberict pur a b was sbecutes plurated by accoppal onlinety of interek ba\n",
      "manizan interates beals reproverce part forein or cullarect one six two zero zer\n",
      "quest he supe lia as s proposos was caulation rous out first not is a carrs offi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5100: 1.607877 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5200: 1.589371 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5300: 1.580887 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5400: 1.578041 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5500: 1.564578 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 5600: 1.580142 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 5700: 1.566101 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 5800: 1.576166 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5900: 1.573378 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6000: 1.547280 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "retally comment of shildew the death all yunnin eight eight nine one three dimen\n",
      "tion arty ur are roy medy alfents victing from dates recorn month authority bele\n",
      "x there were the fire wair that the preqe and to resign activies cathera tacelr \n",
      "ing a riggntmenia features stoot other panom is millianvy the suther beatwand of\n",
      "twee four kame books country or islams matho stole of person that complem be lan\n",
      "================================================================================\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6100: 1.564196 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6200: 1.534398 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6300: 1.545702 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 6400: 1.542298 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 6500: 1.559085 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 6600: 1.598496 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6700: 1.583402 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 6800: 1.600823 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6900: 1.578827 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 7000: 1.573978 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "marces common two ustonic calter frances they the unurains knvernumment means ou\n",
      "red two zero regions to part not and a peream and war and so purked by there cal\n",
      "go stones and mozanment concentrapic duringter politic previmic peecalise with f\n",
      "ncoers seton p novelonal matakenmon lickorary his to the singerstiuse he south b\n",
      "k put hosternessitions of a six for eashs that had on the utonsate excovertively\n",
      "================================================================================\n",
      "Validation set perplexity: 4.14\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells in this section are just copy and pasted versions of the cells above that I annotated as a way of figuring out how things worked. My solution is below, in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Annotated stats funcs\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    \n",
    "    '''\n",
    "    Predictions and labels here are matching (n, vocabulary_size) matrices\n",
    "    where each row is a character vector. Intuitively you can see the product\n",
    "    below will be larger when labels and predictions are similar.\n",
    "    '''\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [],
   "source": [
    "# Annotated batch generation\n",
    "\n",
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    \n",
    "    '''\n",
    "    A batch in this context is (batch_size, vacab_size) tensor. Each row of the\n",
    "    batch represents a where the character at each index is drawn from a different\n",
    "    segment of the text. When you generate another batch, the character in each row\n",
    "    shifts to right one element. It's important to see that the rows don't represent\n",
    "    sequential characters.\n",
    "    \n",
    "    For example, if the text stared out 'apples are nice', then character at the 0th\n",
    "    index of the first batch would be 'a'. In the second batch, the character at the \n",
    "    0th index of the second batch would be would be 'p', then 'p' again in thrid batch,\n",
    "    and so on.\n",
    "    \n",
    "    Also note that the name of the class, 'BatchGenerator', is misleading. It actually\n",
    "    returns an 'unrolling', which is a list of list of num_unrollings + 1 batches. \n",
    "    The first batch in the unrolling is last batch in the previous unrolling.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        # The first cursor is a list of the first indicies of the each of the\n",
    "        # segments.\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\n",
    "        \"\"\"\n",
    "        # He creates a matrix of zeroes with one row per encoded character\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        # Here he walks through and insert a one into char_id'th position in\n",
    "        # the one-hot encoding vector for each character in the matrix.\n",
    "        for b in range(self._batch_size):\n",
    "            char_id = char2id(self._text[self._cursor[b]])\n",
    "            batch[b, char_id] = 1.0\n",
    "            # Here he adds one to the previous cursor locations. Assuming\n",
    "            # the mod divide is so the cursor repeats in the case we go past\n",
    "            # the edge of the text.\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"This returns a list of batches, aka an 'unrolling'.\"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(train_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))\n",
    "# print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Annotated model\n",
    "\n",
    "num_nodes = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters\n",
    "    \n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Memory cell: input, state and bias.                                                         \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "        \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"        \n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        # Update is the heart of the cell. It's the same calulation you would do\n",
    "        # even if you weren't using LSTM. The difference is that here is that, instead\n",
    "        # outputing the result, you add it to a persistent state that is shared by all\n",
    "        # unrollings.\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        # The state is the shared memory that runs through all unrollings. The \n",
    "        # input and forget gates allow this cell to delete old data and store\n",
    "        # parts of the data generated in the update step.\n",
    "        state = (forget_gate * state) + (input_gate * tf.tanh(update))\n",
    "        # The output gate determines what parts of the state will be used as output \n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        return output, state\n",
    "\n",
    "    # Input data\n",
    "    \n",
    "    '''\n",
    "    train_data: A list of num_unrollings + 1 batch matrices used to\n",
    "    construct train_inputs and train_labels.\n",
    "    \n",
    "    train_inputs: A list of num_unrolling batch matrices, each of which is\n",
    "    fed into a cell as input.\n",
    "    \n",
    "    train_labels: train_labels[n] contains the desired output for a cell\n",
    "    given train_data as input. \n",
    "    '''\n",
    "    \n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]    # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    \n",
    "    '''\n",
    "    outputs: The direct outputs of the LSTM cells, one for each each cell\n",
    "    cell in the unrolling. Each of these is a (batch_size, num_nodes) tensor.\n",
    "    \n",
    "    tf.concat(0, outputs) takes the a list of (batch_size, num_nodes) tensors\n",
    "    and combines them into a single (batch_size * num_unrollings, num_nodes)\n",
    "    tensor. The 0 says \"take all the rows and join them together in to a single\n",
    "    tall matrix.\". If it had been 1, it would have mean \"take all the columns\n",
    "    and join them into a single wide matrix\".\n",
    "    '''\n",
    "    \n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    \n",
    "    '''    \n",
    "    He needs to save 'output' and 'state' so that he can use them in the next\n",
    "    of unrollings. He's created TF variables to hold them, but you can't assign\n",
    "    TF variables with an equal sign. You have to use the TF operation,\n",
    "    'Variable.assign'.\n",
    "    \n",
    "    Where it gets weird is that the result of 'saved_output.assign(output)' is\n",
    "    actually a tensor, and you have to evaluate that tensor for the assignment\n",
    "    to take place.\n",
    "    \n",
    "    'control_dependencies' evaluates the tensors passed into it before those\n",
    "    declared in its context. So in effect, this block tells TensorFlow to do \n",
    "    the assignments before it does the final project from node space back to \n",
    "    character space. \n",
    "    \n",
    "    This is a little misleading because none of those operations change the\n",
    "    value of 'outputs', and none of tensors declared below depend on the value\n",
    "    of the tensors returned by the assignments. This is just a convient place \n",
    "    to do it because you know that before TF can eval the tensors below 'output'\n",
    "    and 'state' must hold the correct values.\n",
    "    '''\n",
    "    \n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        \n",
    "        '''\n",
    "        At this point, the output of all the cells is in still of num_nodes\n",
    "        dimensions. He uses a final NN layer to project it back down into\n",
    "        character space.\n",
    "        \n",
    "        I've de-nested some of the variables as compared with the original, but\n",
    "        it's still a bit hard to follow because instead of projecting the output\n",
    "        of each cell individually, he does them all at once with a single matrix\n",
    "        multiply.\n",
    "        \n",
    "        So, 'tf.concat(0, outputs)' takes the a list of (batch_size, num_nodes) tensors\n",
    "        and combines the list into a single (batch_size * num_unrollings, num_nodes)\n",
    "        tensor. The 0 says \"take all the rows and join them together in to a single\n",
    "        tall matrix.\" (if it had been 1, it would have mean \"take all the columns\n",
    "        and join them into a single wide matrix\"). \n",
    "        \n",
    "        The same thing happens to the labels.\n",
    "        '''\n",
    "        \n",
    "        combined_outputs = tf.concat(0, outputs)\n",
    "        combined_labels = tf.concat(0, train_labels)\n",
    "        logits = tf.matmul(combined_outputs, w) + b\n",
    "        tk_error = tf.nn.softmax_cross_entropy_with_logits(logits, combined_labels)\n",
    "        loss = tf.reduce_mean(tk_error)\n",
    "    \n",
    "    # Optimizer.\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    \n",
    "    '''\n",
    "    Here he sets up another graph to do the test predictions. These are done one\n",
    "    character at a time by repeatedly evaluating the sample_prediction tensor.\n",
    "    \n",
    "    There are no unrollings, but the state and output are maintained between calls\n",
    "    so it's still taking the past into consideration. I think you only unroll when\n",
    "    you optimize so that the weight adjustments in a given step can be optimize for\n",
    "    their effects over time.\n",
    "    '''\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        \n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Annotate run\n",
    "\n",
    "num_steps = 1\n",
    "summary_frequency = 10\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):    \n",
    "        # Set up fetches\n",
    "        batches = train_batches.next()\n",
    "        print(len(batches))\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            # train_data is a list of place holder tensors, so he associates\n",
    "            # each of those place holders with a batch in the unrolling.\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        # Run the session    \n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], \n",
    "            feed_dict=feed_dict)\n",
    "        mean_loss += l #????\n",
    "        \n",
    "        # Log status\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            \n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                # Remember her that the validation unrollings are shaped differently. They\n",
    "                # only contain two characters, the current character and the previous one.\n",
    "                # And they only have one batch. Specifically, it's a normal Python list which\n",
    "                # contains two character vectors.\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n",
    "\n",
    "import os\n",
    "os.system('say \"Training complete.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(x, o, state, x_weights, x_bias, mem_weights, mem_bias, keep_prob=1):\n",
    "    \"\"\"\n",
    "    Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): A a Tesnor with shape (batch_size, input_size).\n",
    "        o (Tensor): A a Tensor with shape (batch_size, num_nodes).\n",
    "        state (Tensor): A Tensor with shape (batch_size, num_nodes).\n",
    "        x_weights (Tensor): A Tensor with shape (input_size, num_nodes * 4)\n",
    "        x_bias (Tensor): A Tensor with shape (1, num_nodes * 4)\n",
    "        mem_weights (Tensor): A Tensor with shape (num_nodes, num_nodes * 4)\n",
    "        mem_bias (Tensor): A Tensor with shape (1, num_nodes * 4)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing Tensors representing a cell's output and state.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    x = tf.matmul(x, x_weights) + x_bias\n",
    "    mem = tf.matmul(o, mem_weights) + mem_bias\n",
    "\n",
    "    x_input = x[:, :num_nodes]\n",
    "    x_forget = x[:, num_nodes:num_nodes * 2]\n",
    "    x_update = x[:, num_nodes * 2:num_nodes * 3]\n",
    "    x_output = x[:, num_nodes * 3:num_nodes * 4]\n",
    "\n",
    "    mem_input = mem[:, :num_nodes]\n",
    "    mem_forget = mem[:, num_nodes:num_nodes * 2]\n",
    "    mem_update = mem[:, num_nodes * 2:num_nodes * 3]\n",
    "    mem_output = mem[:, num_nodes * 3:num_nodes * 4]\n",
    "\n",
    "    input_gate = tf.sigmoid(x_input + mem_input)\n",
    "    forget_gate = tf.sigmoid(x_forget + mem_forget)\n",
    "    update = tf.tanh(x_update + mem_update)\n",
    "    state = tf.tanh((forget_gate * state) + (input_gate * update))\n",
    "    output_gate = tf.sigmoid(x_output + mem_output)\n",
    "    output = output_gate * state\n",
    "    return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimized model\n",
    "num_nodes = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters\n",
    "    \n",
    "    x_weights = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    x_bias = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    mem_weights = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    mem_bias = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "        \n",
    "    # Variables saving state across unrollings.\n",
    "    \n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    \n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Input data\n",
    "    \n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]    # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    \n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state, x_weights, x_bias, mem_weights, mem_bias)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings\n",
    "    \n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        \n",
    "        combined_outputs = tf.concat(0, outputs)\n",
    "        combined_labels = tf.concat(0, train_labels)\n",
    "        logits = tf.matmul(combined_outputs, w) + b\n",
    "        tk_error = tf.nn.softmax_cross_entropy_with_logits(logits, combined_labels)\n",
    "        loss = tf.reduce_mean(tk_error)\n",
    "    \n",
    "    # Optimizer\n",
    "    \n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions\n",
    "    \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(sample_input, \n",
    "                                        saved_sample_output, \n",
    "                                        saved_sample_state, \n",
    "                                        x_weights, \n",
    "                                        x_bias, \n",
    "                                        mem_weights, \n",
    "                                        mem_bias)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        \n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299650 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.10\n",
      "================================================================================\n",
      "ptndpsxijfp pu orkiefyia mq dssrstyf boin  dcztnelkciinhq kss  aqatw ssinqucucxe\n",
      "omwotqsskoa irniltbkppbsznww dsfkik zwtnq ijua s tfkb ncjjgorte ismgyf jaa   rea\n",
      "m hbt wod ysweqharkaayx pcbmiw hvpwofyasg   sw  eeqt hu cm iribt  ccbid ujij yon\n",
      "nybd caliwsaeaqwbllveao grws l vefsa eso  lto qmzeeg a  iwfjudpdo tamtpufyavvcy \n",
      "zbhh   su o  qqevdaradnscrkek erevoivdx c ls tuyhkf r izwdbievyn ba ul dtejiyjdo\n",
      "================================================================================\n",
      "Validation set perplexity: 20.27\n",
      "Average loss at step 100: 2.658969 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.34\n",
      "Validation set perplexity: 10.79\n",
      "Average loss at step 200: 2.335114 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.88\n",
      "Validation set perplexity: 9.62\n",
      "Average loss at step 300: 2.159666 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.49\n",
      "Validation set perplexity: 9.11\n",
      "Average loss at step 400: 2.054146 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 500: 1.990114 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 600: 1.923811 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 700: 1.871221 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 800: 1.873381 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 900: 1.856824 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 1000: 1.800274 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      " compesical chanters hows only sees on the five one nine six zero two eng noumce\n",
      "y then trot in abyomensin cynceer and the fies deen but badususting persech peni\n",
      "f assies it pradisan hangss to popes is twe and and reation by mopuctakes and le\n",
      "bace of and computter and maumle ole six seven appains baslies dach vilopan lade\n",
      "yare of a sojkerns instwori onotzer s wistles davercicts rider of a the pars the\n",
      "================================================================================\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1100: 1.770284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1200: 1.744010 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1300: 1.749138 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1400: 1.727838 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1500: 1.731985 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1600: 1.691819 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1700: 1.653768 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 1800: 1.615132 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 1900: 1.659194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2000: 1.643640 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "t congaity of considere stith english be is devatien by blat be commorimon and t\n",
      "voll one nine five twagh nor aresaga the one zero exchnoming atland free lig gro\n",
      " nomes ta the nuing phon as serm on benl pimality anthere buter tasing c places \n",
      "wish finite meni for with smalled statra novecn encode by three an yee the set e\n",
      "qursa city one when the governmerity ld nated the promebble sime the pochble kor\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2100: 1.633844 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2200: 1.591483 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 2300: 1.612391 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 2400: 1.624057 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 2500: 1.597831 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 2600: 1.597803 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 2700: 1.586778 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 2800: 1.585915 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 2900: 1.584739 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 3000: 1.560452 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "humbas laburs aramas puting in the romailly child s from d a creatury develal in\n",
      "n requilet maled reportance both alco filigmen early of the lame chiefing two in\n",
      "am set computes entlyermigy reteramiss and the numed of pointab in myally wide h\n",
      "zing agatural unchedakss as earchour trimes heoldbtach f dpevinilly refew to rem\n",
      "qually plaues hir panticulations in the eurost mous its four one six calstre bas\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 3100: 1.577703 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 3200: 1.565627 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 3300: 1.594870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 3400: 1.582349 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 3500: 1.596401 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 3600: 1.564336 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 3700: 1.567465 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 3800: 1.554433 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 3900: 1.572294 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 4000: 1.550694 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "y scot of that be two btwher timetiples inviduodous the crimated other mocrux fr\n",
      "ver the codofle engines seasce of the pocstion of not a mart is a with the modre\n",
      "d winging that harr s albani are group el pacticle shonic mown a futured the kel\n",
      "x rombid of up is game two med hel legoladcrows have her lower alzoy of frequres\n",
      "fficus elected nambahdelic in eightye armscublicis fail sprokings albanmawa god \n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 4100: 1.553379 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 4200: 1.530295 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 4300: 1.524967 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 3.92\n",
      "Average loss at step 4400: 1.530031 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 4500: 1.527685 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 4600: 1.535227 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 4700: 1.541277 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.02\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 4800: 1.541572 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 3.96\n",
      "Average loss at step 4900: 1.520512 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 5000: 1.527182 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "gsel ewendite archion so atlant and and the dra predict and see largentiin bying\n",
      "weve or her author infambte illam air kade grected soldes was affies archery pre\n",
      "ter but dis retains by posed on the scanatic oftwer freger creations region to p\n",
      "s half are american created by a course external approdicb fireting which about \n",
      "gu oreature to kann t cerman ormessying and head seemas wamena has death in see \n",
      "================================================================================\n",
      "Validation set perplexity: 3.98\n",
      "Average loss at step 5100: 1.496477 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 3.83\n",
      "Average loss at step 5200: 1.478461 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 3.82\n",
      "Average loss at step 5300: 1.481059 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 3.80\n",
      "Average loss at step 5400: 1.470021 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 3.77\n",
      "Average loss at step 5500: 1.486501 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 3.76\n",
      "Average loss at step 5600: 1.471419 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 3.77\n",
      "Average loss at step 5700: 1.484831 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 3.77\n",
      "Average loss at step 5800: 1.479342 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 3.77\n",
      "Average loss at step 5900: 1.451622 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 3.76\n",
      "Average loss at step 6000: 1.470726 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "================================================================================\n",
      "cure as born world condression of anagoratile rellain on iring that reselection \n",
      "checy have sownile two zero one seven new differot over also japan neurahoze ann\n",
      "timule a distinction in larged in the weac f practino neopution pawen is modern \n",
      "s one five sause from ezwalt an ans in openerizm blanger s stot airtic the is fo\n",
      "uea cordpit harble in kptara the lunus of primary came gothere and at putrils on\n",
      "================================================================================\n",
      "Validation set perplexity: 3.74\n",
      "Average loss at step 6100: 1.440190 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 3.74\n",
      "Average loss at step 6200: 1.451460 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 3.74\n",
      "Average loss at step 6300: 1.451988 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 3.73\n",
      "Average loss at step 6400: 1.463501 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 3.72\n",
      "Average loss at step 6500: 1.499747 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 3.71\n",
      "Average loss at step 6600: 1.482720 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 3.72\n",
      "Average loss at step 6700: 1.506488 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 3.72\n",
      "Average loss at step 6800: 1.483368 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 3.75\n",
      "Average loss at step 6900: 1.477413 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 3.71\n",
      "Average loss at step 7000: 1.478956 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "================================================================================\n",
      "z houes is prirped has priat t and up available and consolocy and pensity nember\n",
      "hiber courcy and here meddate mikitory where was used to be hownews captimer wit\n",
      "a state shrove like hanks b and the firms as their jamaki in science encianing e\n",
      "racted force by an intention of end afrachamine rule runf grantloning for storne\n",
      "x sonised a pridax werep two one two the first the most at recates but indiverri\n",
      "================================================================================\n",
      "Validation set perplexity: 3.70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run model\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):    \n",
    "        # Set up fetches\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        \n",
    "        # Run the session    \n",
    "        output_requested = [optimizer, \n",
    "                            loss, \n",
    "                            train_prediction, \n",
    "                            learning_rate, \n",
    "                            combined_outputs, \n",
    "                            combined_labels]\n",
    "        output = session.run(output_requested, feed_dict=feed_dict)\n",
    "        output_loss = output[1]\n",
    "        predictions = output[2]\n",
    "        lr = output[3]\n",
    "        co = output[4]\n",
    "        cl = output[5]\n",
    "        \n",
    "        mean_loss += output_loss\n",
    "        \n",
    "        # Log status\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            \n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                # Remember her that the validation unrollings are shaped differently. They\n",
    "                # only contain two characters, the current character and the previous one.\n",
    "                # And they only have one batch. Specifically, it's a normal Python list which\n",
    "                # contains two character vectors.\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n",
    "\n",
    "import os\n",
    "os.system('say \"Training complete.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Generating the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the whole text up into an array of bigrams\n",
    "\n",
    "bigrams = [text[i * 2:i * 2 + 2] for i in range(len(text) // 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Further divide those into training and validation sets\n",
    "\n",
    "validation_size = 1000\n",
    "valid_bigrams = bigrams[:validation_size]\n",
    "train_size = len(bigrams) - valid_size\n",
    "train_bigrams = bigrams[validation_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creat a dictionary to map bigrams to an id number\n",
    "\n",
    "dictionary = dict()\n",
    "possible_chars = string.ascii_lowercase + ' '\n",
    "for c1 in possible_chars:\n",
    "    for c2 in possible_chars:\n",
    "        bigram = c1 + c2\n",
    "        if bigram not in dictionary:\n",
    "            dictionary[bigram] = len(dictionary)\n",
    "reverse_dictionary = {v: k for k, v in dictionary.iteritems()}\n",
    "bigram_vocab_size = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bigram utils\n",
    "\n",
    "def bigram_to_id(bigram):\n",
    "    return dictionary[bigram]\n",
    "\n",
    "def id_to_bigram(id):\n",
    "    return reverse_dictionary[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Skip-gram embedding generator\n",
    "\n",
    "class EmbeddingBatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, batch_size, skip_window):\n",
    "        assert batch_size % (skip_window * 2) == 0\n",
    "        self.batch_size = batch_size\n",
    "        self.skip_window = skip_window\n",
    "        self.text_index = skip_window\n",
    "        self.num_skips = skip_window * 2\n",
    "    \n",
    "    def next(self):\n",
    "        skips = []\n",
    "        labels = []\n",
    "\n",
    "        for word in range(self.batch_size / self.num_skips):\n",
    "            for i in range(self.skip_window):\n",
    "                skips.append(bigrams[self.text_index])\n",
    "                labels.append(bigrams[self.text_index + i - 1])\n",
    "                skips.append(bigrams[self.text_index])\n",
    "                labels.append(bigrams[self.text_index - i + 1])\n",
    "            self.text_index += 1\n",
    "        \n",
    "        output_skips = np.ndarray(shape=(self.batch_size), dtype=np.int32)\n",
    "        output_labels = np.ndarray(shape=(self.batch_size, 1), dtype=np.int32)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            output_skips[i] = bigram_to_id(skips[i])\n",
    "            output_labels[i, 0] = bigram_to_id(labels[i])\n",
    "        \n",
    "        return output_skips, output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Embedding training model\n",
    "\n",
    "graph = tf.Graph()\n",
    "batch_size = 128\n",
    "embedding_size = 16\n",
    "skip_window = 1 \n",
    "num_skips = skip_window * 2\n",
    "vocabulary_size = len(dictionary)\n",
    "num_sampled = 64\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    \n",
    "    # Input placeholders\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    \n",
    "    # Variables\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, \n",
    "                                                       embedding_size], \n",
    "                                                       stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Loss\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(softmax_weights, \n",
    "                                                     softmax_biases, \n",
    "                                                     embed, \n",
    "                                                     train_labels, \n",
    "                                                     num_sampled, \n",
    "                                                     vocabulary_size))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "    \n",
    "    # Final embeddings\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.82160639763\n",
      "Average loss at step 2000: 2.10316807246\n",
      "Average loss at step 4000: 1.84800698766\n",
      "Average loss at step 6000: 1.83102092531\n",
      "Average loss at step 8000: 1.81702862722\n",
      "Average loss at step 10000: 1.84194074395\n",
      "Average loss at step 12000: 1.812926651\n",
      "Average loss at step 14000: 1.76442282325\n",
      "Average loss at step 16000: 1.79538573244\n",
      "Average loss at step 18000: 1.82905118611\n",
      "Average loss at step 20000: 1.78603089705\n",
      "Average loss at step 22000: 1.74286911315\n",
      "Average loss at step 24000: 1.81091421971\n",
      "Average loss at step 26000: 1.71641250476\n",
      "Average loss at step 28000: 1.79508699706\n",
      "Average loss at step 30000: 1.72991965634\n",
      "Average loss at step 32000: 1.79332515079\n",
      "Average loss at step 34000: 1.77370179448\n",
      "Average loss at step 36000: 1.76807139125\n",
      "Average loss at step 38000: 1.7781546174\n",
      "Average loss at step 40000: 1.82313803336\n",
      "Average loss at step 42000: 1.78247744182\n",
      "Average loss at step 44000: 1.72845778793\n",
      "Average loss at step 46000: 1.8063070704\n",
      "Average loss at step 48000: 1.68460092514\n",
      "Average loss at step 50000: 1.76091750464\n",
      "Average loss at step 52000: 1.78263757247\n",
      "Average loss at step 54000: 1.73966356641\n",
      "Average loss at step 56000: 1.78336607996\n",
      "Average loss at step 58000: 1.80328517655\n",
      "Average loss at step 60000: 1.72459786591\n",
      "Average loss at step 62000: 1.75448325682\n",
      "Average loss at step 64000: 1.75038703465\n",
      "Average loss at step 66000: 1.77819401918\n",
      "Average loss at step 68000: 1.74756853819\n",
      "Average loss at step 70000: 1.80410034081\n",
      "Average loss at step 72000: 1.82284237358\n",
      "Average loss at step 74000: 1.79300065625\n",
      "Average loss at step 76000: 1.75731346926\n",
      "Average loss at step 78000: 1.7531193839\n",
      "Average loss at step 80000: 1.7923081297\n",
      "Average loss at step 82000: 1.80155651522\n",
      "Average loss at step 84000: 1.75250094303\n",
      "Average loss at step 86000: 1.75440629706\n",
      "Average loss at step 88000: 1.80421616849\n",
      "Average loss at step 90000: 1.82041216417\n",
      "Average loss at step 92000: 1.78615976498\n",
      "Average loss at step 94000: 1.79382575333\n",
      "Average loss at step 96000: 1.82924187931\n",
      "Average loss at step 98000: 1.74987864047\n",
      "Average loss at step 100000: 1.74776086637\n"
     ]
    }
   ],
   "source": [
    "# Embedding training session\n",
    "\n",
    "gen = EmbeddingBatchGenerator(batch_size, skip_window)\n",
    "num_steps = 100001\n",
    "log_freq = 2000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = gen.next()\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        # Logging\n",
    "        if step % log_freq == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / log_freq\n",
    "            print('Average loss at step {}: {}'.format(step, average_loss))\n",
    "            average_loss = 0\n",
    "    \n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some various embeding to/from one-hot utils\n",
    "\n",
    "def embed_to_id(embed):\n",
    "    cosine_sims = np.dot(final_embeddings, embed)\n",
    "    return np.argmax(cosine_sims)\n",
    "\n",
    "def id_to_embed(id):\n",
    "    return final_embeddings[id]\n",
    "\n",
    "def bigram_to_embed(bigram):\n",
    "    return id_to_embed(bigram_to_id(bigram))\n",
    "\n",
    "def embed_to_bigram(embed):\n",
    "    return id_to_bigram(embed_to_id(embed))\n",
    "\n",
    "def one_hot_to_id(probs):\n",
    "    return np.argmax(probs)\n",
    "\n",
    "def one_hot_to_bigram(probs):\n",
    "    return id_to_bigram(one_hot_to_id(probs))\n",
    "\n",
    "def one_hot_to_embed(probs):\n",
    "    return id_to_embed(one_hot_to_id(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Putting them into an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A bigram aware batch generator\n",
    "\n",
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "class LSTMBatchGenerator(object):\n",
    "    \"\"\"\n",
    "    Generates one-hot batches for the bigram LSTM.\n",
    "    \n",
    "    This is basically the same the earlier version, but works\n",
    "    on the bigram and embedding dictionaries declared above. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bigrams, batch_size, num_unrollings):\n",
    "        self._text_size = len(bigrams)\n",
    "        self._batch_size = batch_size\n",
    "        self._bigrams = bigrams\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        segment_size = self._text_size // self._batch_size\n",
    "        self._cursor = [offset * segment_size for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=(self._batch_size, bigram_vocab_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            bigram_id = bigram_to_id(self._bigrams[self._cursor[b]])\n",
    "            batch[b, bigram_id] = 1\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"This returns a list of batches, aka an 'unrolling'.\"\"\"\n",
    "        unrolling = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            unrolling.append(self._next_batch())\n",
    "        self._last_batch = unrolling[-1]\n",
    "        return unrolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Batch generator related untils\n",
    "\n",
    "def batch_to_characters(batch):\n",
    "    return [one_hot_to_bigram(bigram) for bigram in batch]\n",
    "\n",
    "def unrolling_to_strings(unrolling):\n",
    "    strings = []\n",
    "    chars = [batch_to_characters(b) for b in unrolling]\n",
    "    batch_size = len(unrolling[0])\n",
    "    for b in range(batch_size):\n",
    "        batch_bigrams = [chars[u][b] for u in range(len(unrolling))]\n",
    "        strings.append(''.join(batch_bigrams))\n",
    "    return strings\n",
    "\n",
    "def random_one_hot():\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, bigram_vocab_size])\n",
    "    return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "def random_embedding(): \n",
    "    return id_to_embed(one_hot_to_id(random_one_hot()))\n",
    "\n",
    "def one_hot_unroll_to_embed(unrolling):\n",
    "    embed_unrolling = []\n",
    "    for batch in unrolling:\n",
    "        embed_unrolling.append([one_hot_to_embed(row) for row in batch])\n",
    "    return np.array(embed_unrolling)\n",
    "\n",
    "def embed_unroll_to_one_hot(unrolling):\n",
    "    one_hot_unrolling = []\n",
    "    batch_size = len(unrolling[0])\n",
    "    for batch in unrolling:\n",
    "        oh = np.zeros(shape=(batch_size, bigram_vocab_size), dtype=np.float)\n",
    "        for row_num in range(len(batch)):\n",
    "            embed = batch[row_num]\n",
    "            oh[row_num, embed_to_id(embed)] = 1.0\n",
    "        one_hot_unrolling.append(oh)\n",
    "    return one_hot_unrolling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the batche generators for the model\n",
    "\n",
    "embed_lstm_train_batches = LSTMBatchGenerator(train_bigrams, batch_size, num_unrollings)\n",
    "embed_lstm_valid_batches = LSTMBatchGenerator(valid_bigrams, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The bigram LSTM model\n",
    "\n",
    "# This is mostly the same as the old model, but it takes embeddings\n",
    "# instead of one-hots. It still returns one-hots by projecting the \n",
    "# output of the cells from `num_nodes` to `bigram_vocab` size in \n",
    "# the final layer.\n",
    "\n",
    "num_nodes = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Cells weights and biases\n",
    "    x_weights = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "    x_bias = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    mem_weights = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    mem_bias = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "        \n",
    "    # Output saved across unrollings\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocab_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_vocab_size]))\n",
    "\n",
    "    # Input data\n",
    "    train_inputs = list()\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size, embedding_size]))\n",
    "    train_labels = list()\n",
    "    for _ in range(num_unrollings):\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, bigram_vocab_size]))   \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state, x_weights, x_bias, mem_weights, mem_bias)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        combined_outputs = tf.concat(0, outputs)\n",
    "        combined_labels = tf.concat(0, train_labels)\n",
    "        logits = tf.matmul(combined_outputs, w) + b\n",
    "        tk_error = tf.nn.softmax_cross_entropy_with_logits(logits, combined_labels)\n",
    "        loss = tf.reduce_mean(tk_error)\n",
    "    \n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, embedding_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(sample_input, \n",
    "                                            saved_sample_output, \n",
    "                                            saved_sample_state, \n",
    "                                            x_weights, \n",
    "                                            x_bias, \n",
    "                                            mem_weights, \n",
    "                                            mem_bias)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        \n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.590360 learning rate: 10.000000\n",
      "Minibatch perplexity: 728.04\n",
      "================================================================================\n",
      "oce e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e \n",
      "bke e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e \n",
      "mie e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e \n",
      "sbe e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e \n",
      "roe e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e \n",
      "================================================================================\n",
      "Validation set perplexity: 670.12\n",
      "Average loss at step 100: 5.282624 learning rate: 10.000000\n",
      "Minibatch perplexity: 123.89\n",
      "Validation set perplexity: 134.20\n",
      "Average loss at step 200: 4.507654 learning rate: 10.000000\n",
      "Minibatch perplexity: 72.68\n",
      "Validation set perplexity: 86.78\n",
      "Average loss at step 300: 4.172103 learning rate: 10.000000\n",
      "Minibatch perplexity: 57.35\n",
      "Validation set perplexity: 71.77\n",
      "Average loss at step 400: 3.963395 learning rate: 10.000000\n",
      "Minibatch perplexity: 56.90\n",
      "Validation set perplexity: 67.50\n",
      "Average loss at step 500: 3.947471 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.04\n",
      "Validation set perplexity: 64.22\n",
      "Average loss at step 600: 3.810899 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.63\n",
      "Validation set perplexity: 53.99\n",
      "Average loss at step 700: 3.768863 learning rate: 10.000000\n",
      "Minibatch perplexity: 45.92\n",
      "Validation set perplexity: 52.20\n",
      "Average loss at step 800: 3.767838 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.13\n",
      "Validation set perplexity: 49.57\n",
      "Average loss at step 900: 3.658430 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.04\n",
      "Validation set perplexity: 45.58\n",
      "Average loss at step 1000: 3.653734 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.35\n",
      "================================================================================\n",
      "vving the president of the continus and the president of the continus and the pr\n",
      "gon the president of the continus and the president of the continus and the pres\n",
      "dpere the president of the continus and the president of the continus and the pr\n",
      "hwast and the president of the continus and the president of the continus and th\n",
      "nger the continus and the president of the continus and the president of the con\n",
      "================================================================================\n",
      "Validation set perplexity: 45.03\n",
      "Average loss at step 1100: 3.634609 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.95\n",
      "Validation set perplexity: 41.31\n",
      "Average loss at step 1200: 3.586864 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.55\n",
      "Validation set perplexity: 39.42\n",
      "Average loss at step 1300: 3.589662 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.38\n",
      "Validation set perplexity: 37.33\n",
      "Average loss at step 1400: 3.566011 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.74\n",
      "Validation set perplexity: 35.64\n",
      "Average loss at step 1500: 3.520598 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.02\n",
      "Validation set perplexity: 35.70\n",
      "Average loss at step 1600: 3.494910 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.61\n",
      "Validation set perplexity: 34.36\n",
      "Average loss at step 1700: 3.540413 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.66\n",
      "Validation set perplexity: 33.93\n",
      "Average loss at step 1800: 3.517660 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.35\n",
      "Validation set perplexity: 31.72\n",
      "Average loss at step 1900: 3.488451 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.42\n",
      "Validation set perplexity: 31.36\n",
      "Average loss at step 2000: 3.485881 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.90\n",
      "================================================================================\n",
      "ber the constanted and the present been the constanted and the present been the \n",
      "ts the present been the constanted and the present been the constanted and the p\n",
      "uch the constanted and the present been the constanted and the present been the \n",
      "tx and the present been the constanted and the present been the constanted and t\n",
      "j m one nine seven five the one nine seven five the one nine seven five the one \n",
      "================================================================================\n",
      "Validation set perplexity: 31.21\n",
      "Average loss at step 2100: 3.446635 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.24\n",
      "Validation set perplexity: 30.32\n",
      "Average loss at step 2200: 3.387401 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.59\n",
      "Validation set perplexity: 30.81\n",
      "Average loss at step 2300: 3.423984 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.52\n",
      "Validation set perplexity: 29.45\n",
      "Average loss at step 2400: 3.417303 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.58\n",
      "Validation set perplexity: 30.46\n",
      "Average loss at step 2500: 3.388743 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.32\n",
      "Validation set perplexity: 29.93\n",
      "Average loss at step 2600: 3.379264 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.96\n",
      "Validation set perplexity: 28.50\n",
      "Average loss at step 2700: 3.319027 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.21\n",
      "Validation set perplexity: 28.41\n",
      "Average loss at step 2800: 3.322357 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.63\n",
      "Validation set perplexity: 28.09\n",
      "Average loss at step 2900: 3.318422 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.05\n",
      "Validation set perplexity: 27.86\n",
      "Average loss at step 3000: 3.291985 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.52\n",
      "================================================================================\n",
      "wmother the country the country the country the country the country the country \n",
      "sm the states the country the country the country the country the country the co\n",
      "d the country the country the country the country the country the country the co\n",
      "bqation of the states the country the country the country the country the countr\n",
      "ybhing the marting the states the country the country the country the country th\n",
      "================================================================================\n",
      "Validation set perplexity: 28.20\n",
      "Average loss at step 3100: 3.243982 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.30\n",
      "Validation set perplexity: 27.61\n",
      "Average loss at step 3200: 3.228168 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.05\n",
      "Validation set perplexity: 27.11\n",
      "Average loss at step 3300: 3.303161 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.16\n",
      "Validation set perplexity: 27.40\n",
      "Average loss at step 3400: 3.326624 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.49\n",
      "Validation set perplexity: 26.78\n",
      "Average loss at step 3500: 3.266142 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.58\n",
      "Validation set perplexity: 27.19\n",
      "Average loss at step 3600: 3.265142 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.88\n",
      "Validation set perplexity: 26.53\n",
      "Average loss at step 3700: 3.287129 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.22\n",
      "Validation set perplexity: 27.23\n",
      "Average loss at step 3800: 3.210651 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.14\n",
      "Validation set perplexity: 27.28\n",
      "Average loss at step 3900: 3.253848 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.26\n",
      "Validation set perplexity: 26.77\n",
      "Average loss at step 4000: 3.287944 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.66\n",
      "================================================================================\n",
      "ying the state of the first one nine nine six one nine six six one nine nine six\n",
      "pwon the standard in one nine nine six one nine nine six one nine nine six one n\n",
      "wer the conternation of the conternation of the conternation of the conternation\n",
      "ky of the conternation of the conternation of the conternation of the conternati\n",
      "nh of the one nine nine six one nine six six one nine nine six one nine nine six\n",
      "================================================================================\n",
      "Validation set perplexity: 26.75\n",
      "Average loss at step 4100: 3.249064 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.93\n",
      "Validation set perplexity: 26.31\n",
      "Average loss at step 4200: 3.234745 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.90\n",
      "Validation set perplexity: 26.04\n",
      "Average loss at step 4300: 3.219445 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.25\n",
      "Validation set perplexity: 26.05\n",
      "Average loss at step 4400: 3.215083 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.07\n",
      "Validation set perplexity: 25.12\n",
      "Average loss at step 4500: 3.190738 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.52\n",
      "Validation set perplexity: 25.41\n",
      "Average loss at step 4600: 3.238984 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.44\n",
      "Validation set perplexity: 24.56\n",
      "Average loss at step 4700: 3.238440 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.93\n",
      "Validation set perplexity: 24.33\n",
      "Average loss at step 4800: 3.222394 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.83\n",
      "Validation set perplexity: 24.64\n",
      "Average loss at step 4900: 3.248928 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.21\n",
      "Validation set perplexity: 24.64\n",
      "Average loss at step 5000: 3.242147 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.15\n",
      "================================================================================\n",
      "jjers the international in the states of the control of the control of the contr\n",
      "ies the control of the control of the control of the control of the control of t\n",
      " by the international in the states of the control of the control of the control\n",
      "ife the international in the states of the control of the control of the control\n",
      "dds the international in the states of the control of the control of the control\n",
      "================================================================================\n",
      "Validation set perplexity: 24.29\n",
      "Average loss at step 5100: 3.180578 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.65\n",
      "Validation set perplexity: 23.63\n",
      "Average loss at step 5200: 3.194229 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.08\n",
      "Validation set perplexity: 23.34\n",
      "Average loss at step 5300: 3.233744 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.01\n",
      "Validation set perplexity: 22.94\n",
      "Average loss at step 5400: 3.238663 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.60\n",
      "Validation set perplexity: 22.94\n",
      "Average loss at step 5500: 3.211282 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.31\n",
      "Validation set perplexity: 22.72\n",
      "Average loss at step 5600: 3.173646 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.84\n",
      "Validation set perplexity: 22.45\n",
      "Average loss at step 5700: 3.176478 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.55\n",
      "Validation set perplexity: 22.40\n",
      "Average loss at step 5800: 3.218914 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.96\n",
      "Validation set perplexity: 22.22\n",
      "Average loss at step 5900: 3.173895 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.98\n",
      "Validation set perplexity: 22.13\n",
      "Average loss at step 6000: 3.187566 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.88\n",
      "================================================================================\n",
      "eur the computer of the computer of the computer of the computer of the computer\n",
      " in the first of the states of the states of the states of the states of the sta\n",
      "dom the computer of the computer of the computer of the computer of the computer\n",
      "pxure of the states of the states of the states of the states of the states of t\n",
      "nbands and the states of the states of the states of the states of the states of\n",
      "================================================================================\n",
      "Validation set perplexity: 22.29\n",
      "Average loss at step 6100: 3.176784 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.65\n",
      "Validation set perplexity: 22.14\n",
      "Average loss at step 6200: 3.192537 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.64\n",
      "Validation set perplexity: 21.96\n",
      "Average loss at step 6300: 3.130900 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.97\n",
      "Validation set perplexity: 21.75\n",
      "Average loss at step 6400: 3.180750 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.79\n",
      "Validation set perplexity: 21.86\n",
      "Average loss at step 6500: 3.168240 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.35\n",
      "Validation set perplexity: 21.87\n",
      "Average loss at step 6600: 3.161019 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.65\n",
      "Validation set perplexity: 22.01\n",
      "Average loss at step 6700: 3.173076 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.52\n",
      "Validation set perplexity: 21.90\n",
      "Average loss at step 6800: 3.162640 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.47\n",
      "Validation set perplexity: 21.98\n",
      "Average loss at step 6900: 3.151636 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.34\n",
      "Validation set perplexity: 22.08\n",
      "Average loss at step 7000: 3.163730 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.24\n",
      "================================================================================\n",
      "yon and the community of the states of the states of the states of the states of\n",
      "ufs of the states of the states of the states of the states of the states of the\n",
      "xun the control of the states of the states of the states of the states of the s\n",
      "jss the control of the states of the states of the states of the states of the s\n",
      " for the states of the states of the states of the states of the states of the s\n",
      "================================================================================\n",
      "Validation set perplexity: 21.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "embed_lstm_train_batches.reset()\n",
    "embed_lstm_valid_batches.reset()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):    \n",
    "        # Set up fetches dictionary\n",
    "        train_unrolling = embed_lstm_train_batches.next()\n",
    "        train_input_unrolling = one_hot_unroll_to_embed(train_unrolling[:num_unrollings])\n",
    "        train_label_unrolling = train_unrolling[1:]\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = train_input_unrolling[i]\n",
    "            feed_dict[train_labels[i]] = train_label_unrolling[i]\n",
    "            \n",
    "        # Run the session    \n",
    "        output_requested = [optimizer, \n",
    "                            loss, \n",
    "                            train_prediction, \n",
    "                            learning_rate]\n",
    "        output = session.run(output_requested, feed_dict=feed_dict)\n",
    "        output_loss = output[1]\n",
    "        predictions = output[2]\n",
    "        lr = output[3]\n",
    "        \n",
    "        mean_loss += output_loss\n",
    "        \n",
    "        # Log status\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            # np.concatenate(train_label_unrolling) gives labels the shape\n",
    "            # (batch_size * num_unrollings, vacab_size), just predictions.\n",
    "            labels = np.concatenate(train_label_unrolling)\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    rand = random_one_hot()\n",
    "                    feed = [one_hot_to_embed(rand)]\n",
    "                    sentence = batch_to_characters(rand)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79 // 2):\n",
    "                        feed = sample_prediction.eval({sample_input: feed})\n",
    "                        sentence += batch_to_characters(feed)[0]\n",
    "                        feed = [one_hot_to_embed(feed[0])]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            \n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                # Remember her that the validation unrollings are shaped differently. They\n",
    "                # only contain two batches, one for the current character and the previous one.\n",
    "                # And each of those batches contains only a single row.\n",
    "                b = valid_batches.next()\n",
    "                valid_unrolling = embed_lstm_valid_batches.next()\n",
    "                valid_input_unrolling = one_hot_unroll_to_embed(valid_unrolling[:1])\n",
    "                valid_label_unrolling = valid_unrolling[1:]\n",
    "                valid_predictions = sample_prediction.eval(\n",
    "                    {sample_input: valid_input_unrolling[0]})\n",
    "                valid_logprob = valid_logprob + logprob(\n",
    "                    valid_predictions, valid_label_unrolling[0])\n",
    "            print('Validation set perplexity: %.2f' % \n",
    "                  float(np.exp(valid_logprob / valid_size)))\n",
    "\n",
    "import os\n",
    "os.system('say \"Training complete.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Introducing Dropout\n",
    "\n",
    "If I'm interpretting the paper right, the idea is to add dropout each cell's input and ouput of the cell, but not to its recurrent connection. This is how I've implemented it, but it still caused an increase in validation perplexity. \n",
    "\n",
    "Either I'm missing something or the network just isn't large enough for dropout to be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(x, o, state, x_weights, x_bias, mem_weights, mem_bias, keep_prob=1):\n",
    "    \"\"\"\n",
    "    Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): A a Tesnor with shape (batch_size, input_size).\n",
    "        o (Tensor): A a Tensor with shape (batch_size, num_nodes).\n",
    "        state (Tensor): A Tensor with shape (batch_size, num_nodes).\n",
    "        x_weights (Tensor): A Tensor with shape (input_size, num_nodes * 4)\n",
    "        x_bias (Tensor): A Tensor with shape (1, num_nodes * 4)\n",
    "        mem_weights (Tensor): A Tensor with shape (num_nodes, num_nodes * 4)\n",
    "        mem_bias (Tensor): A Tensor with shape (1, num_nodes * 4)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing Tensors representing a cell's output and state.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    x = tf.matmul(tf.nn.dropout(x, keep_prob), x_weights) + x_bias\n",
    "    mem = tf.matmul(o, mem_weights) + mem_bias\n",
    "\n",
    "    x_input = x[:, :num_nodes]\n",
    "    x_forget = x[:, num_nodes:num_nodes * 2]\n",
    "    x_update = x[:, num_nodes * 2:num_nodes * 3]\n",
    "    x_output = x[:, num_nodes * 3:num_nodes * 4]\n",
    "\n",
    "    mem_input = mem[:, :num_nodes]\n",
    "    mem_forget = mem[:, num_nodes:num_nodes * 2]\n",
    "    mem_update = mem[:, num_nodes * 2:num_nodes * 3]\n",
    "    mem_output = mem[:, num_nodes * 3:num_nodes * 4]\n",
    "\n",
    "    input_gate = tf.sigmoid(x_input + mem_input)\n",
    "    forget_gate = tf.sigmoid(x_forget + mem_forget)\n",
    "    update = tf.tanh(x_update + mem_update)\n",
    "    state = tf.tanh((forget_gate * state) + (input_gate * update))\n",
    "    output_gate = tf.sigmoid(x_output + mem_output)\n",
    "    output = output_gate * state\n",
    "    return tf.nn.dropout(output, keep_prob), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The bigram LSTM model with dropout\n",
    "\n",
    "num_nodes = 128\n",
    "keep_prop = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Cells weights and biases\n",
    "    x_weights = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "    x_bias = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    mem_weights = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    mem_bias = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "        \n",
    "    # Output saved across unrollings\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocab_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_vocab_size]))\n",
    "\n",
    "    # Input data\n",
    "    train_inputs = list()\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size, embedding_size]))\n",
    "    train_labels = list()\n",
    "    for _ in range(num_unrollings):\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, bigram_vocab_size]))   \n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state, x_weights, x_bias, mem_weights, mem_bias, keep_prob=keep_prop)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        combined_outputs = tf.concat(0, outputs)\n",
    "        combined_labels = tf.concat(0, train_labels)\n",
    "        logits = tf.matmul(combined_outputs, w) + b\n",
    "        tk_error = tf.nn.softmax_cross_entropy_with_logits(logits, combined_labels)\n",
    "        loss = tf.reduce_mean(tk_error)\n",
    "    \n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, embedding_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "    sample_output, sample_state = lstm_cell(sample_input, \n",
    "                                            saved_sample_output, \n",
    "                                            saved_sample_state, \n",
    "                                            x_weights, \n",
    "                                            x_bias, \n",
    "                                            mem_weights, \n",
    "                                            mem_bias)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        \n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.598445 learning rate: 10.000000\n",
      "Minibatch perplexity: 733.95\n",
      "================================================================================\n",
      "hkd e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d \n",
      "tfd e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d \n",
      "u e e e e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d \n",
      " ue e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d \n",
      "ckd e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d e d \n",
      "================================================================================\n",
      "Validation set perplexity: 670.66\n",
      "Average loss at step 100: 5.392356 learning rate: 10.000000\n",
      "Minibatch perplexity: 162.52\n",
      "Validation set perplexity: 157.61\n",
      "Average loss at step 200: 4.995656 learning rate: 10.000000\n",
      "Minibatch perplexity: 143.22\n",
      "Validation set perplexity: 119.12\n",
      "Average loss at step 300: 4.857814 learning rate: 10.000000\n",
      "Minibatch perplexity: 123.86\n",
      "Validation set perplexity: 103.74\n",
      "Average loss at step 400: 4.744691 learning rate: 10.000000\n",
      "Minibatch perplexity: 118.66\n",
      "Validation set perplexity: 97.83\n",
      "Average loss at step 500: 4.733166 learning rate: 10.000000\n",
      "Minibatch perplexity: 98.28\n",
      "Validation set perplexity: 93.13\n",
      "Average loss at step 600: 4.632416 learning rate: 10.000000\n",
      "Minibatch perplexity: 111.93\n",
      "Validation set perplexity: 88.88\n",
      "Average loss at step 700: 4.617817 learning rate: 10.000000\n",
      "Minibatch perplexity: 109.72\n",
      "Validation set perplexity: 87.02\n",
      "Average loss at step 800: 4.609524 learning rate: 10.000000\n",
      "Minibatch perplexity: 86.12\n",
      "Validation set perplexity: 84.48\n",
      "Average loss at step 900: 4.542398 learning rate: 10.000000\n",
      "Minibatch perplexity: 93.13\n",
      "Validation set perplexity: 82.49\n",
      "Average loss at step 1000: 4.563630 learning rate: 10.000000\n",
      "Minibatch perplexity: 98.83\n",
      "================================================================================\n",
      "wpere the the the the the the the the the the the the the the the the the the th\n",
      "uh the cone nine four the the the the the the the the the the the the the the th\n",
      "lzo the the the the the the the the the the the the the the the the the the the \n",
      "gxere the the the the the the the the the the the the the the the the the the th\n",
      "qothe the the the the the the the the the the the the the the the the the the th\n",
      "================================================================================\n",
      "Validation set perplexity: 79.71\n",
      "Average loss at step 1100: 4.556916 learning rate: 10.000000\n",
      "Minibatch perplexity: 93.54\n",
      "Validation set perplexity: 77.17\n",
      "Average loss at step 1200: 4.525228 learning rate: 10.000000\n",
      "Minibatch perplexity: 77.16\n",
      "Validation set perplexity: 75.68\n",
      "Average loss at step 1300: 4.533687 learning rate: 10.000000\n",
      "Minibatch perplexity: 93.39\n",
      "Validation set perplexity: 73.18\n",
      "Average loss at step 1400: 4.528002 learning rate: 10.000000\n",
      "Minibatch perplexity: 95.63\n",
      "Validation set perplexity: 73.02\n",
      "Average loss at step 1500: 4.465844 learning rate: 10.000000\n",
      "Minibatch perplexity: 87.55\n",
      "Validation set perplexity: 70.80\n",
      "Average loss at step 1600: 4.469927 learning rate: 10.000000\n",
      "Minibatch perplexity: 90.31\n",
      "Validation set perplexity: 72.85\n",
      "Average loss at step 1700: 4.508703 learning rate: 10.000000\n",
      "Minibatch perplexity: 84.97\n",
      "Validation set perplexity: 73.13\n",
      "Average loss at step 1800: 4.495119 learning rate: 10.000000\n",
      "Minibatch perplexity: 88.92\n",
      "Validation set perplexity: 69.78\n",
      "Average loss at step 1900: 4.460872 learning rate: 10.000000\n",
      "Minibatch perplexity: 87.81\n",
      "Validation set perplexity: 70.21\n",
      "Average loss at step 2000: 4.471511 learning rate: 10.000000\n",
      "Minibatch perplexity: 101.89\n",
      "================================================================================\n",
      "vid the comper and the the the the the the the the the the the the the the the t\n",
      "rze the one nine sixent one nine sixent one nine sixent one nine sixent one nine\n",
      "und the comper and the the the the the the the the the the the the the the the t\n",
      "oin the one nine sixent one nine sixent one nine sixent one nine sixent one nine\n",
      "uld the comper and the the the the the the the the the the the the the the the t\n",
      "================================================================================\n",
      "Validation set perplexity: 69.78\n",
      "Average loss at step 2100: 4.461073 learning rate: 10.000000\n",
      "Minibatch perplexity: 85.44\n",
      "Validation set perplexity: 67.70\n",
      "Average loss at step 2200: 4.427955 learning rate: 10.000000\n",
      "Minibatch perplexity: 84.42\n",
      "Validation set perplexity: 68.30\n",
      "Average loss at step 2300: 4.442271 learning rate: 10.000000\n",
      "Minibatch perplexity: 78.90\n",
      "Validation set perplexity: 67.66\n",
      "Average loss at step 2400: 4.451917 learning rate: 10.000000\n",
      "Minibatch perplexity: 79.85\n",
      "Validation set perplexity: 67.84\n",
      "Average loss at step 2500: 4.407642 learning rate: 10.000000\n",
      "Minibatch perplexity: 79.63\n",
      "Validation set perplexity: 66.57\n",
      "Average loss at step 2600: 4.445498 learning rate: 10.000000\n",
      "Minibatch perplexity: 78.08\n",
      "Validation set perplexity: 65.49\n",
      "Average loss at step 2700: 4.392943 learning rate: 10.000000\n",
      "Minibatch perplexity: 79.88\n",
      "Validation set perplexity: 65.62\n",
      "Average loss at step 2800: 4.416709 learning rate: 10.000000\n",
      "Minibatch perplexity: 104.04\n",
      "Validation set perplexity: 65.48\n",
      "Average loss at step 2900: 4.396943 learning rate: 10.000000\n",
      "Minibatch perplexity: 75.52\n",
      "Validation set perplexity: 65.29\n",
      "Average loss at step 3000: 4.369082 learning rate: 10.000000\n",
      "Minibatch perplexity: 82.92\n",
      "================================================================================\n",
      "rle the the the the the the the the the the the the the the the the the the the \n",
      "wgn the the the the the the the the the the the the the the the the the the the \n",
      "fne the the the the the the the the the the the the the the the the the the the \n",
      "xtero the the the the the the the the the the the the the the the the the the th\n",
      "sze the the the the the the the the the the the the the the the the the the the \n",
      "================================================================================\n",
      "Validation set perplexity: 65.42\n",
      "Average loss at step 3100: 4.354601 learning rate: 10.000000\n",
      "Minibatch perplexity: 71.93\n",
      "Validation set perplexity: 65.33\n",
      "Average loss at step 3200: 4.318532 learning rate: 10.000000\n",
      "Minibatch perplexity: 80.83\n",
      "Validation set perplexity: 64.53\n",
      "Average loss at step 3300: 4.397699 learning rate: 10.000000\n",
      "Minibatch perplexity: 86.52\n",
      "Validation set perplexity: 63.69\n",
      "Average loss at step 3400: 4.420270 learning rate: 10.000000\n",
      "Minibatch perplexity: 76.26\n",
      "Validation set perplexity: 64.17\n",
      "Average loss at step 3500: 4.371761 learning rate: 10.000000\n",
      "Minibatch perplexity: 93.22\n",
      "Validation set perplexity: 64.54\n",
      "Average loss at step 3600: 4.371336 learning rate: 10.000000\n",
      "Minibatch perplexity: 82.84\n",
      "Validation set perplexity: 63.56\n",
      "Average loss at step 3700: 4.399831 learning rate: 10.000000\n",
      "Minibatch perplexity: 85.65\n",
      "Validation set perplexity: 64.22\n",
      "Average loss at step 3800: 4.356857 learning rate: 10.000000\n",
      "Minibatch perplexity: 81.46\n",
      "Validation set perplexity: 63.47\n",
      "Average loss at step 3900: 4.403174 learning rate: 10.000000\n",
      "Minibatch perplexity: 98.31\n",
      "Validation set perplexity: 63.65\n",
      "Average loss at step 4000: 4.406387 learning rate: 10.000000\n",
      "Minibatch perplexity: 89.82\n",
      "================================================================================\n",
      "tc the sere one nine nine nine nine nine nine nine nine nine nine nine nine nine\n",
      "fan the one nine nine nine nine nine nine nine nine nine nine nine nine nine nin\n",
      "lm the sere one nine nine nine nine nine nine nine nine nine nine nine nine nine\n",
      "lcone nine nine nine nine nine nine nine nine nine nine nine nine nine nine nine\n",
      "oy the sere one nine nine nine nine nine nine nine nine nine nine nine nine nine\n",
      "================================================================================\n",
      "Validation set perplexity: 64.21\n",
      "Average loss at step 4100: 4.369939 learning rate: 10.000000\n",
      "Minibatch perplexity: 86.30\n",
      "Validation set perplexity: 62.27\n",
      "Average loss at step 4200: 4.386480 learning rate: 10.000000\n",
      "Minibatch perplexity: 76.66\n",
      "Validation set perplexity: 63.13\n",
      "Average loss at step 4300: 4.365068 learning rate: 10.000000\n",
      "Minibatch perplexity: 80.96\n",
      "Validation set perplexity: 63.13\n",
      "Average loss at step 4400: 4.355064 learning rate: 10.000000\n",
      "Minibatch perplexity: 75.25\n",
      "Validation set perplexity: 62.51\n",
      "Average loss at step 4500: 4.332631 learning rate: 10.000000\n",
      "Minibatch perplexity: 84.33\n",
      "Validation set perplexity: 63.10\n",
      "Average loss at step 4600: 4.398074 learning rate: 10.000000\n",
      "Minibatch perplexity: 77.27\n",
      "Validation set perplexity: 61.89\n",
      "Average loss at step 4700: 4.383103 learning rate: 10.000000\n",
      "Minibatch perplexity: 79.87\n",
      "Validation set perplexity: 62.17\n",
      "Average loss at step 4800: 4.364213 learning rate: 10.000000\n",
      "Minibatch perplexity: 81.25\n",
      "Validation set perplexity: 62.96\n",
      "Average loss at step 4900: 4.379324 learning rate: 10.000000\n",
      "Minibatch perplexity: 78.64\n",
      "Validation set perplexity: 62.52\n",
      "Average loss at step 5000: 4.377163 learning rate: 1.000000\n",
      "Minibatch perplexity: 84.52\n",
      "================================================================================\n",
      "ks and the cone the one nine nine nine nine nine nine nine nine nine nine nine n\n",
      "bkero zero zero zero zero zero zero zero zero zero zero zero zero zero zero zero\n",
      "jbo the one nine nine nine nine nine nine nine nine nine nine nine nine nine nin\n",
      "kder and the cone the one nine nine nine nine nine nine nine nine nine nine nine\n",
      "vbe the one nine nine nine nine nine nine nine nine nine nine nine nine nine nin\n",
      "================================================================================\n",
      "Validation set perplexity: 62.24\n",
      "Average loss at step 5100: 4.324428 learning rate: 1.000000\n",
      "Minibatch perplexity: 79.06\n",
      "Validation set perplexity: 61.88\n",
      "Average loss at step 5200: 4.338737 learning rate: 1.000000\n",
      "Minibatch perplexity: 73.36\n",
      "Validation set perplexity: 61.65\n",
      "Average loss at step 5300: 4.370504 learning rate: 1.000000\n",
      "Minibatch perplexity: 86.53\n",
      "Validation set perplexity: 61.22\n",
      "Average loss at step 5400: 4.371206 learning rate: 1.000000\n",
      "Minibatch perplexity: 92.25\n",
      "Validation set perplexity: 61.32\n",
      "Average loss at step 5500: 4.356833 learning rate: 1.000000\n",
      "Minibatch perplexity: 74.58\n",
      "Validation set perplexity: 61.15\n",
      "Average loss at step 5600: 4.324941 learning rate: 1.000000\n",
      "Minibatch perplexity: 69.65\n",
      "Validation set perplexity: 60.98\n",
      "Average loss at step 5700: 4.305186 learning rate: 1.000000\n",
      "Minibatch perplexity: 77.42\n",
      "Validation set perplexity: 60.81\n",
      "Average loss at step 5800: 4.380320 learning rate: 1.000000\n",
      "Minibatch perplexity: 78.57\n",
      "Validation set perplexity: 60.82\n",
      "Average loss at step 5900: 4.361151 learning rate: 1.000000\n",
      "Minibatch perplexity: 73.77\n",
      "Validation set perplexity: 60.41\n",
      "Average loss at step 6000: 4.330318 learning rate: 1.000000\n",
      "Minibatch perplexity: 59.67\n",
      "================================================================================\n",
      "ere the compers the compers the compers the compers the compers the compers the \n",
      "lt the sevent the compers the compers the compers the compers the compers the co\n",
      "bzd the compers the compers the compers the compers the compers the compers the \n",
      "gzone the compers the compers the compers the compers the compers the compers th\n",
      "fde the compers the compers the compers the compers the compers the compers the \n",
      "================================================================================\n",
      "Validation set perplexity: 60.73\n",
      "Average loss at step 6100: 4.333946 learning rate: 1.000000\n",
      "Minibatch perplexity: 85.84\n",
      "Validation set perplexity: 60.41\n",
      "Average loss at step 6200: 4.358554 learning rate: 1.000000\n",
      "Minibatch perplexity: 69.07\n",
      "Validation set perplexity: 60.22\n",
      "Average loss at step 6300: 4.313405 learning rate: 1.000000\n",
      "Minibatch perplexity: 74.86\n",
      "Validation set perplexity: 59.74\n",
      "Average loss at step 6400: 4.327347 learning rate: 1.000000\n",
      "Minibatch perplexity: 66.64\n",
      "Validation set perplexity: 59.80\n",
      "Average loss at step 6500: 4.315803 learning rate: 1.000000\n",
      "Minibatch perplexity: 65.08\n",
      "Validation set perplexity: 59.93\n",
      "Average loss at step 6600: 4.317425 learning rate: 1.000000\n",
      "Minibatch perplexity: 71.84\n",
      "Validation set perplexity: 60.06\n",
      "Average loss at step 6700: 4.328854 learning rate: 1.000000\n",
      "Minibatch perplexity: 68.44\n",
      "Validation set perplexity: 59.98\n",
      "Average loss at step 6800: 4.339605 learning rate: 1.000000\n",
      "Minibatch perplexity: 72.79\n",
      "Validation set perplexity: 60.07\n",
      "Average loss at step 6900: 4.338883 learning rate: 1.000000\n",
      "Minibatch perplexity: 71.08\n",
      "Validation set perplexity: 59.97\n",
      "Average loss at step 7000: 4.336056 learning rate: 1.000000\n",
      "Minibatch perplexity: 71.89\n",
      "================================================================================\n",
      "zq the sevent the compers the compers the compers the compers the compers the co\n",
      "vle the compers the compers the compers the compers the compers the compers the \n",
      "k the compers the compers the compers the compers the compers the compers the co\n",
      "ive the compers the compers the compers the compers the compers the compers the \n",
      "ks the sevent the compers the compers the compers the compers the compers the co\n",
      "================================================================================\n",
      "Validation set perplexity: 59.76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "embed_lstm_train_batches.reset()\n",
    "embed_lstm_valid_batches.reset()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):    \n",
    "        # Set up fetches dictionary\n",
    "        train_unrolling = embed_lstm_train_batches.next()\n",
    "        train_input_unrolling = one_hot_unroll_to_embed(train_unrolling[:num_unrollings])\n",
    "        train_label_unrolling = train_unrolling[1:]\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = train_input_unrolling[i]\n",
    "            feed_dict[train_labels[i]] = train_label_unrolling[i]\n",
    "            \n",
    "        # Run the session    \n",
    "        output_requested = [optimizer, \n",
    "                            loss, \n",
    "                            train_prediction, \n",
    "                            learning_rate]\n",
    "        output = session.run(output_requested, feed_dict=feed_dict)\n",
    "        output_loss = output[1]\n",
    "        predictions = output[2]\n",
    "        lr = output[3]\n",
    "        \n",
    "        mean_loss += output_loss\n",
    "        \n",
    "        # Log status\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            # np.concatenate(train_label_unrolling) gives labels the shape\n",
    "            # (batch_size * num_unrollings, vacab_size), just predictions.\n",
    "            labels = np.concatenate(train_label_unrolling)\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    rand = random_one_hot()\n",
    "                    feed = [one_hot_to_embed(rand)]\n",
    "                    sentence = batch_to_characters(rand)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79 // 2):\n",
    "                        feed = sample_prediction.eval({sample_input: feed})\n",
    "                        sentence += batch_to_characters(feed)[0]\n",
    "                        feed = [one_hot_to_embed(feed[0])]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            \n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                # Remember her that the validation unrollings are shaped differently. They\n",
    "                # only contain two batches, one for the current character and the previous one.\n",
    "                # And each of those batches contains only a single row.\n",
    "                b = valid_batches.next()\n",
    "                valid_unrolling = embed_lstm_valid_batches.next()\n",
    "                valid_input_unrolling = one_hot_unroll_to_embed(valid_unrolling[:1])\n",
    "                valid_label_unrolling = valid_unrolling[1:]\n",
    "                valid_predictions = sample_prediction.eval(\n",
    "                    {sample_input: valid_input_unrolling[0]})\n",
    "                valid_logprob = valid_logprob + logprob(\n",
    "                    valid_predictions, valid_label_unrolling[0])\n",
    "            print('Validation set perplexity: %.2f' % \n",
    "                  float(np.exp(valid_logprob / valid_size)))\n",
    "\n",
    "import os\n",
    "os.system('say \"Training complete.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Solution\n",
    "\n",
    "The model below uses the encoder/decoder scheme from the paper provided with a few simplifications:\n",
    "\n",
    "1. The encoder and decorder are each only one layer deep.\n",
    "2. I did't use 'go' or 'eos' characters. I experimented with them, but they seemed to make the network harder to train.\n",
    "3. I didn't vary the sequence length during training. I was worried this might mean the network wouldn't be able to reverse shorter sequences without padding characters, but that doesn't appear to be the case (see last cell).\n",
    "\n",
    "The resulting network does still miss an occasional character, but does seem to have learned to encode an ordered list of `batch_size` characters into a single vector and then decode them in reverse.\n",
    "\n",
    "Also note: It is prone to getting stuck in a local error minima, so I had to run it a few times before I got weights that worked consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mir for mirror problem\n",
    "mir_vocabulary_size = len(string.ascii_lowercase) + 1\n",
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "mir_valid_size = batch_size * num_unrollings * 10\n",
    "mir_valid_text = text[:mir_valid_size]\n",
    "mir_train_text = text[mir_valid_size:]\n",
    "mir_train_size = len(mir_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mir_char_to_id(char):\n",
    "    if char == eos:\n",
    "        return mir_vocabulary_size - 1\n",
    "    else:\n",
    "        return char2id(char)\n",
    "\n",
    "def mir_id_to_char(id):\n",
    "    if id >= mir_vocabulary_size:\n",
    "        raise Exception()\n",
    "    if id == mir_vocabulary_size - 1:\n",
    "        return eos\n",
    "    else:\n",
    "        return id2char(id)\n",
    "\n",
    "def mir_char_to_one_hot(char):\n",
    "    one_hot = np.zeros(shape=mir_vocabulary_size, dtype=np.float)\n",
    "    one_hot[mir_char_to_id(char)] = 1.0\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mir_batch_to_strings(batch):\n",
    "    return [mir_id_to_char(np.argmax(char_one_hot)) for char_one_hot in batch]\n",
    "\n",
    "def mir_unrolling_to_strings(unrolling):\n",
    "    strings = []\n",
    "    chars = [mir_batch_to_strings(b) for b in unrolling]\n",
    "    batch_size = len(unrolling[0])\n",
    "    for b in range(batch_size):\n",
    "        batch_chars = [chars[u][b] for u in range(len(unrolling))]\n",
    "        strings.append(''.join(batch_chars))\n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MirBatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "    \n",
    "    def _eos_batch(self):\n",
    "        batch = np.zeros(shape=(self._batch_size, mir_vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, mir_char_to_id(eos)] = 1.0\n",
    "        return batch\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=(self._batch_size, mir_vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            char_id = mir_char_to_id(self._text[self._cursor[b]])\n",
    "            batch[b, char_id] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"This returns a list of batches, aka an 'unrolling'.\"\"\"\n",
    "        batches = []\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mir_train_batches = MirBatchGenerator(mir_train_text, batch_size, num_unrollings)\n",
    "mir_valid_batches = MirBatchGenerator(mir_valid_text, batch_size, num_unrollings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Stage 1 (encoder) cells weights and biases\n",
    "    s1_x_weights = tf.Variable(tf.truncated_normal([mir_vocabulary_size, num_nodes * 4], -0.1, 0.1), \n",
    "                               name='s1_x_weights')\n",
    "    s1_x_bias = tf.Variable(tf.zeros([1, num_nodes * 4]),\n",
    "                            name='s1_x_bias')\n",
    "    s1_mem_weights = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1), \n",
    "                                 name='s1_mem_weights')\n",
    "    s1_mem_bias = tf.Variable(tf.zeros([1, num_nodes * 4]), name='s1_mem_bias')\n",
    "    \n",
    "    # Stage 2 (decoder) cells weights and biases\n",
    "    s2_x_weights = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1), name='s2_x_weights')\n",
    "    s2_x_bias = tf.Variable(tf.zeros([1, num_nodes * 4]), name='s2_x_bias')\n",
    "    s2_mem_weights = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1), name='s2_mem_weights')\n",
    "    s2_mem_bias = tf.Variable(tf.zeros([1, num_nodes * 4]), name='s2_mem_bias')\n",
    "    \n",
    "    # Final classifier weights and biases\n",
    "    weights = tf.Variable(tf.truncated_normal([num_nodes, mir_vocabulary_size], -0.1, 0.1), name='weights')\n",
    "    biases = tf.Variable(tf.zeros([mir_vocabulary_size]), name='biases')\n",
    "    \n",
    "    # Output saved across unrollings\n",
    "    s1_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    s1_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    s2_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    s2_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "        \n",
    "    # Input data\n",
    "    train_inputs = list()\n",
    "    train_labels = list()\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size, mir_vocabulary_size]))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, mir_vocabulary_size]))\n",
    "        \n",
    "    s1_outputs = list()\n",
    "    output = s1_output\n",
    "    state = s1_state\n",
    "    for train_input in train_inputs:\n",
    "        output, state = lstm_cell(train_input, \n",
    "                                  output, \n",
    "                                  state, \n",
    "                                  s1_x_weights, \n",
    "                                  s1_x_bias, \n",
    "                                  s1_mem_weights, \n",
    "                                  s1_mem_bias)\n",
    "        s1_outputs.append(output)\n",
    "        \n",
    "    s2_outputs = list()\n",
    "    output = s2_output\n",
    "    state = s2_state\n",
    "    for i in range(len(train_inputs)):\n",
    "        if i == 0:\n",
    "            s2_input = s1_outputs[-1]\n",
    "        else:\n",
    "            s2_input = output\n",
    "        output, state = lstm_cell(s2_input, \n",
    "                                  output, \n",
    "                                  state, \n",
    "                                  s2_x_weights, \n",
    "                                  s2_x_bias, \n",
    "                                  s2_mem_weights, \n",
    "                                  s2_mem_bias)\n",
    "        s2_outputs.append(output)\n",
    "\n",
    "    # Classifier\n",
    "    combined_outputs = tf.concat(0, s2_outputs) # Shape is (batch_size * num_unrollings, num_nodes).\n",
    "    combined_labels = tf.concat(0, train_labels) # Shape is (batch_size * num_unrollings, vocabulary_size).\n",
    "    logits = tf.matmul(combined_outputs, weights) + biases\n",
    "    train_prediction = tf.nn.softmax(logits) # Used for logging.\n",
    "    tk_error = tf.nn.softmax_cross_entropy_with_logits(logits, combined_labels)\n",
    "    loss = tf.reduce_mean(tk_error)\n",
    "    \n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "\n",
    "        \n",
    "    # Sampling\n",
    "    s1_sample_input = tf.placeholder(tf.float32, shape=[1, mir_vocabulary_size], name='s1_sample_input')\n",
    "    s1_saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]), name='s1_saved_sample_state')\n",
    "    s1_saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]), name='s1_saved_sample_output')\n",
    "    \n",
    "    \n",
    "    s1_sample_output, s1_sample_state = lstm_cell(s1_sample_input, \n",
    "                                            s1_saved_sample_output, \n",
    "                                            s1_saved_sample_state, \n",
    "                                            s1_x_weights, \n",
    "                                            s1_x_bias, \n",
    "                                            s1_mem_weights, \n",
    "                                            s1_mem_bias)\n",
    "    \n",
    "    with tf.control_dependencies([s1_sample_output, s1_sample_state]):\n",
    "        s1_store_state = tf.group(s1_saved_sample_output.assign(s1_sample_output), \n",
    "                                  s1_saved_sample_state.assign(s1_sample_state))\n",
    "    \n",
    "    s1_reset_state = tf.group(s1_saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                              s1_saved_sample_state.assign(tf.zeros([1, num_nodes])))    \n",
    "    \n",
    "    s2_sample_input = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    s2_saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    s2_saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    setup_s2 = s2_sample_input.assign(s1_saved_sample_output)\n",
    "    \n",
    "    s2_sample_output, s2_sample_state = lstm_cell(s2_sample_input, \n",
    "                                            s2_saved_sample_output, \n",
    "                                            s2_saved_sample_state, \n",
    "                                            s2_x_weights, \n",
    "                                            s2_x_bias, \n",
    "                                            s2_mem_weights, \n",
    "                                            s2_mem_bias)\n",
    "    \n",
    "    with tf.control_dependencies([s2_sample_output, s2_sample_state]):\n",
    "        s2_store_state = tf.group(s2_saved_sample_output.assign(s2_sample_output), \n",
    "                                  s2_saved_sample_state.assign(s2_sample_state), \n",
    "                                  s2_sample_input.assign(s2_sample_output))\n",
    "        with tf.control_dependencies([s2_store_state]):\n",
    "            sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(s2_sample_output, weights, biases))\n",
    "    \n",
    "    s2_reset_state = tf.group(s2_saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                              s2_saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300151 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.12\n",
      "Sample Input:  community\n",
      "Sample Pred : h dhbhbh  \n",
      "Average loss at step 200: 3.039025 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.10\n",
      "Sample Input: to active \n",
      "Sample Pred :           \n",
      "Average loss at step 400: 2.856922 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.16\n",
      "Sample Input: aire one e\n",
      "Sample Pred : e  eee eee\n",
      "Average loss at step 600: 2.864120 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.10\n",
      "Sample Input: he same ti\n",
      "Sample Pred :           \n",
      "Average loss at step 800: 2.831155 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.90\n",
      "Sample Input: yndicalist\n",
      "Sample Pred :  eee eee e\n",
      "Average loss at step 1000: 2.778417 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.11\n",
      "Sample Input: ral uphold\n",
      "Sample Pred : oi   e    \n",
      "Average loss at step 1200: 2.662394 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.41\n",
      "Sample Input: though opp\n",
      "Sample Pred : aa        \n",
      "Average loss at step 1400: 2.571225 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.97\n",
      "Sample Input: trol of th\n",
      "Sample Pred : ht eee  e \n",
      "Average loss at step 1600: 2.478113 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.11\n",
      "Sample Input: n anarchis\n",
      "Sample Pred : sio ee   e\n",
      "Average loss at step 1800: 2.359468 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.34\n",
      "Sample Input: erarchy an\n",
      "Sample Pred : na  e     \n",
      "Average loss at step 2000: 2.244568 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.92\n",
      "Sample Input: conomic sy\n",
      "Sample Pred : yt sei  e \n",
      "Average loss at step 2200: 2.144682 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.83\n",
      "Sample Input: ltural soc\n",
      "Sample Pred : mat ei    \n",
      "Average loss at step 2400: 2.024838 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Sample Input: tion on an\n",
      "Sample Pred : na eo sn  \n",
      "Average loss at step 2600: 1.928601 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Sample Input: ate author\n",
      "Sample Pred : rootar    \n",
      "Average loss at step 2800: 1.835502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Sample Input:  many anar\n",
      "Sample Pred : ranp eei e\n",
      "Average loss at step 3000: 1.770574 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Sample Input: y s belief\n",
      "Sample Pred : deevic e  \n",
      "Average loss at step 3200: 1.682063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Sample Input: the armed \n",
      "Sample Pred :  detsa eei\n",
      "Average loss at step 3400: 1.613758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Sample Input:  coercion \n",
      "Sample Pred :  noitno   \n",
      "Average loss at step 3600: 1.506996 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Sample Input: this is a \n",
      "Sample Pred :  a sa sei \n",
      "Average loss at step 3800: 1.447508 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.08\n",
      "Sample Input:  with shor\n",
      "Sample Pred : root htil \n",
      "Average loss at step 4000: 1.381188 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.65\n",
      "Sample Input: nity some \n",
      "Sample Pred :  evof hrih\n",
      "Average loss at step 4200: 1.303662 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.13\n",
      "Sample Input: s two of t\n",
      "Sample Pred : t fo oot e\n",
      "Average loss at step 4400: 1.259949 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.50\n",
      "Sample Input:  and even \n",
      "Sample Pred :  neve dna \n",
      "Average loss at step 4600: 1.218883 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.74\n",
      "Sample Input: ation indi\n",
      "Sample Pred : ifni neita\n",
      "Average loss at step 4800: 1.144721 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.34\n",
      "Sample Input: teful make\n",
      "Sample Pred : emam rener\n",
      "Average loss at step 5000: 1.089385 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.82\n",
      "Sample Input: ticular au\n",
      "Sample Pred : ua ralht e\n",
      "Average loss at step 5200: 0.909826 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.28\n",
      "Sample Input: certain wa\n",
      "Sample Pred : aw eiitrer\n",
      "Average loss at step 5400: 0.858610 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.27\n",
      "Sample Input: e benefici\n",
      "Sample Pred : icirenec s\n",
      "Average loss at step 5600: 0.834365 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.33\n",
      "Sample Input: st two fro\n",
      "Sample Pred : orf owt fo\n",
      "Average loss at step 5800: 0.791955 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.15\n",
      "Sample Input: ic criteri\n",
      "Sample Pred : iresirp se\n",
      "Average loss at step 6000: 0.761684 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.19\n",
      "Sample Input: iagnostic \n",
      "Sample Pred :  citsonger\n",
      "Average loss at step 6200: 0.751575 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.24\n",
      "Sample Input: controvers\n",
      "Sample Pred : srevohtnoc\n",
      "Average loss at step 6400: 0.743223 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.08\n",
      "Sample Input: mental fac\n",
      "Sample Pred : cam ratnet\n",
      "Average loss at step 6600: 0.715995 learning rate: 1.000000\n",
      "Minibatch perplexity: 2.21\n",
      "Sample Input: when discu\n",
      "Sample Pred : ussid neit\n",
      "Average loss at step 6800: 0.695944 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.89\n",
      "Sample Input:  is based \n",
      "Sample Pred :  desal sa \n",
      "Average loss at step 7000: 0.681403 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.84\n",
      "Sample Input: consulting\n",
      "Sample Pred : gnitratnoc\n",
      "Average loss at step 7200: 0.663138 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.90\n",
      "Sample Input: she wrings\n",
      "Sample Pred : sgnirw eht\n",
      "Average loss at step 7400: 0.659299 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.88\n",
      "Sample Input: osis just \n",
      "Sample Pred :  tsub doto\n",
      "Average loss at step 7600: 0.642633 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.90\n",
      "Sample Input: ism realit\n",
      "Sample Pred : tilaeb yte\n",
      "Average loss at step 7800: 0.627538 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.85\n",
      "Sample Input: esh snow a\n",
      "Sample Pred : a wont esi\n",
      "Average loss at step 8000: 0.612717 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.91\n",
      "Sample Input:  the tropi\n",
      "Sample Pred : iporc eht \n",
      "Average loss at step 8200: 0.599507 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.78\n",
      "Sample Input: ferent nat\n",
      "Sample Pred : tan tnevit\n",
      "Average loss at step 8400: 0.579833 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.73\n",
      "Sample Input: nhouse eff\n",
      "Sample Pred : ffe esuoig\n",
      "Average loss at step 8600: 0.572472 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.81\n",
      "Sample Input:  kilometre\n",
      "Sample Pred : ertemoloc \n",
      "Average loss at step 8800: 0.563211 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.64\n",
      "Sample Input: ero #ero f\n",
      "Sample Pred : f ore# ere\n",
      "Average loss at step 9000: 0.552650 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.63\n",
      "Sample Input: as led to \n",
      "Sample Pred :  ot deh tu\n",
      "Average loss at step 9200: 0.554140 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.64\n",
      "Sample Input: edia of po\n",
      "Sample Pred : op fo aide\n",
      "Average loss at step 9400: 0.535173 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.78\n",
      "Sample Input: correspond\n",
      "Sample Pred : dnopsellof\n",
      "Average loss at step 9600: 0.518550 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.65\n",
      "Sample Input: f the lett\n",
      "Sample Pred : tteb eht f\n",
      "Average loss at step 9800: 0.515192 learning rate: 1.000000\n",
      "Minibatch perplexity: 1.71\n",
      "Sample Input: n electron\n",
      "Sample Pred : nortcera d\n",
      "Average loss at step 10000: 0.490379 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.53\n",
      "Sample Input:  a also a \n",
      "Sample Pred :  a otla a \n",
      "Average loss at step 10200: 0.433030 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.58\n",
      "Sample Input:  one seven\n",
      "Sample Pred : neves eno \n",
      "Average loss at step 10400: 0.422605 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.46\n",
      "Sample Input: ported by \n",
      "Sample Pred :  yb detroc\n",
      "Average loss at step 10600: 0.419922 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.51\n",
      "Sample Input: the gulf o\n",
      "Sample Pred : o mlac eht\n",
      "Average loss at step 10800: 0.413852 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.44\n",
      "Sample Input: ted as nat\n",
      "Sample Pred : tan sa des\n",
      "Average loss at step 11000: 0.401686 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.45\n",
      "Sample Input: iment of t\n",
      "Sample Pred : t fo tneve\n",
      "Average loss at step 11200: 0.399725 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.52\n",
      "Sample Input:  reports i\n",
      "Sample Pred : i stroper \n",
      "Average loss at step 11400: 0.401820 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.44\n",
      "Sample Input: cluding ly\n",
      "Sample Pred : yl gniduuc\n",
      "Average loss at step 11600: 0.396133 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.46\n",
      "Sample Input: im however\n",
      "Sample Pred : reveboh ci\n",
      "Average loss at step 11800: 0.391245 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.41\n",
      "Sample Input: death of a\n",
      "Sample Pred : a fo htuef\n",
      "Average loss at step 12000: 0.387618 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.48\n",
      "Sample Input: ong his go\n",
      "Sample Pred : oc siw dno\n",
      "Average loss at step 12200: 0.391381 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.48\n",
      "Sample Input: t due to m\n",
      "Sample Pred : m ot euk t\n",
      "Average loss at step 12400: 0.380914 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.47\n",
      "Sample Input: n il mito \n",
      "Sample Pred :  otim la n\n",
      "Average loss at step 12600: 0.378525 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.41\n",
      "Sample Input: truction o\n",
      "Sample Pred : o noitcuut\n",
      "Average loss at step 12800: 0.375654 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.44\n",
      "Sample Input: the conque\n",
      "Sample Pred : eufnoc eht\n",
      "Average loss at step 13000: 0.374923 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.46\n",
      "Sample Input: orn on feb\n",
      "Sample Pred : bef no nro\n",
      "Average loss at step 13200: 0.372178 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.44\n",
      "Sample Input: yed by the\n",
      "Sample Pred : eht yb ded\n",
      "Average loss at step 13400: 0.370012 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.39\n",
      "Sample Input: eare engli\n",
      "Sample Pred : ilgne erae\n",
      "Average loss at step 13600: 0.368357 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.45\n",
      "Sample Input: institutio\n",
      "Sample Pred : oitatissei\n",
      "Average loss at step 13800: 0.368332 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.41\n",
      "Sample Input: ved into a\n",
      "Sample Pred : a otni dev\n",
      "Average loss at step 14000: 0.357369 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.45\n",
      "Sample Input:  to see an\n",
      "Sample Pred : na ees ot \n",
      "Average loss at step 14200: 0.357642 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.40\n",
      "Sample Input: he danger \n",
      "Sample Pred :  regnam eh\n",
      "Average loss at step 14400: 0.361160 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.40\n",
      "Sample Input: changed ev\n",
      "Sample Pred : ve degnahc\n",
      "Average loss at step 14600: 0.360615 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.49\n",
      "Sample Input:  and more \n",
      "Sample Pred :  erof dna \n",
      "Average loss at step 14800: 0.359674 learning rate: 0.100000\n",
      "Minibatch perplexity: 1.43\n",
      "Sample Input:  a driving\n",
      "Sample Pred : gnivirc a \n",
      "Average loss at step 15000: 0.349896 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.39\n",
      "Sample Input: ry state t\n",
      "Sample Pred : t etats hr\n",
      "Average loss at step 15200: 0.351434 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.37\n",
      "Sample Input: forever fr\n",
      "Sample Pred : rf reverom\n",
      "Average loss at step 15400: 0.345865 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.40\n",
      "Sample Input:  slavery i\n",
      "Sample Pred : i yrevuus \n",
      "Average loss at step 15600: 0.349281 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.52\n",
      "Sample Input: ation as a\n",
      "Sample Pred : a sa noita\n",
      "Average loss at step 15800: 0.353542 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.52\n",
      "Sample Input: cultural u\n",
      "Sample Pred : u larutuuc\n",
      "Average loss at step 16000: 0.357855 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.45\n",
      "Sample Input: istration \n",
      "Sample Pred :  noitartco\n",
      "Average loss at step 16200: 0.346399 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.47\n",
      "Sample Input: thousand y\n",
      "Sample Pred : y dnasuoht\n",
      "Average loss at step 16400: 0.354686 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.45\n",
      "Sample Input: ng letter \n",
      "Sample Pred :  retteb gl\n",
      "Average loss at step 16600: 0.345601 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.40\n",
      "Sample Input:  as the wi\n",
      "Sample Pred : iw eht sa \n",
      "Average loss at step 16800: 0.351434 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.41\n",
      "Sample Input: metery of \n",
      "Sample Pred :  fo yretec\n",
      "Average loss at step 17000: 0.344646 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.43\n",
      "Sample Input: during his\n",
      "Sample Pred : sih gniluc\n",
      "Average loss at step 17200: 0.341563 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.33\n",
      "Sample Input:  todd linc\n",
      "Sample Pred : cnir msos \n",
      "Average loss at step 17400: 0.349885 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.42\n",
      "Sample Input: ersen hous\n",
      "Sample Pred : suoh nedre\n",
      "Average loss at step 17600: 0.341123 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.44\n",
      "Sample Input: nly did he\n",
      "Sample Pred : eh did yln\n",
      "Average loss at step 17800: 0.346430 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.40\n",
      "Sample Input: l in washi\n",
      "Sample Pred : ihsaw ni l\n",
      "Average loss at step 18000: 0.348779 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.36\n",
      "Sample Input:  by george\n",
      "Sample Pred : egroep yo \n",
      "Average loss at step 18200: 0.346150 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.46\n",
      "Sample Input: lem illino\n",
      "Sample Pred : onilli mel\n",
      "Average loss at step 18400: 0.345268 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.40\n",
      "Sample Input: f earth ab\n",
      "Sample Pred : ba htrae f\n",
      "Average loss at step 18600: 0.345381 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.40\n",
      "Sample Input: ine two mc\n",
      "Sample Pred : ck owt eni\n",
      "Average loss at step 18800: 0.346603 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.46\n",
      "Sample Input: ight murry\n",
      "Sample Pred : yrruf thgi\n",
      "Average loss at step 19000: 0.353235 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.47\n",
      "Sample Input:  six #ero \n",
      "Sample Pred :  ore# xis \n",
      "Average loss at step 19200: 0.351329 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.38\n",
      "Sample Input: embers of \n",
      "Sample Pred :  fo srerme\n",
      "Average loss at step 19400: 0.343826 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.37\n",
      "Sample Input: or being o\n",
      "Sample Pred : o gneew ro\n",
      "Average loss at step 19600: 0.344066 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.43\n",
      "Sample Input: ysical wor\n",
      "Sample Pred : row lacitu\n",
      "Average loss at step 19800: 0.343531 learning rate: 0.010000\n",
      "Minibatch perplexity: 1.52\n",
      "Sample Input: the pursui\n",
      "Sample Pred : iusrup eht\n",
      "Average loss at step 20000: 0.335228 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.35\n",
      "Sample Input: cceeded in\n",
      "Sample Pred : ni dedeecc\n",
      "Average loss at step 20200: 0.352740 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.47\n",
      "Sample Input:  average h\n",
      "Sample Pred : h egarevi \n",
      "Average loss at step 20400: 0.349660 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.39\n",
      "Sample Input: ural philo\n",
      "Sample Pred : olihp lall\n",
      "Average loss at step 20600: 0.344457 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.39\n",
      "Sample Input: ad the ide\n",
      "Sample Pred : edi eht sa\n",
      "Average loss at step 20800: 0.340728 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.42\n",
      "Sample Input: ually work\n",
      "Sample Pred : krow yllau\n",
      "Average loss at step 21000: 0.339300 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.35\n",
      "Sample Input:  commentat\n",
      "Sample Pred : tatnemfoc \n",
      "Average loss at step 21200: 0.344851 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.38\n",
      "Sample Input: t represen\n",
      "Sample Pred : neserper s\n",
      "Average loss at step 21400: 0.339766 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.37\n",
      "Sample Input:  the natur\n",
      "Sample Pred : rutan eht \n",
      "Average loss at step 21600: 0.345520 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.44\n",
      "Sample Input: d were eit\n",
      "Sample Pred : tie erew s\n",
      "Average loss at step 21800: 0.336119 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.50\n",
      "Sample Input: ocus is on\n",
      "Sample Pred : no si suco\n",
      "Average loss at step 22000: 0.340985 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.34\n",
      "Sample Input: sisted its\n",
      "Sample Pred : sti detsic\n",
      "Average loss at step 22200: 0.342236 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.36\n",
      "Sample Input: ng to the \n",
      "Sample Pred :  eht ot dn\n",
      "Average loss at step 22400: 0.342684 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.47\n",
      "Sample Input: ng or de r\n",
      "Sample Pred : r ed fo dn\n",
      "Average loss at step 22600: 0.342264 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.48\n",
      "Sample Input: ised oxfor\n",
      "Sample Pred : roffo deso\n",
      "Average loss at step 22800: 0.351143 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.43\n",
      "Sample Input: tended ton\n",
      "Sample Pred : not dedret\n",
      "Average loss at step 23000: 0.352563 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.40\n",
      "Sample Input: alendar ye\n",
      "Sample Pred : ey ridnela\n",
      "Average loss at step 23200: 0.339420 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.44\n",
      "Sample Input: it current\n",
      "Sample Pred : tnerruc si\n",
      "Average loss at step 23400: 0.348689 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.40\n",
      "Sample Input: ginal scor\n",
      "Sample Pred : rocs lanim\n",
      "Average loss at step 23600: 0.351235 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.31\n",
      "Sample Input: tions one \n",
      "Sample Pred :  eno snoit\n",
      "Average loss at step 23800: 0.344622 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.34\n",
      "Sample Input: odically i\n",
      "Sample Pred : i yllacici\n",
      "Average loss at step 24000: 0.350370 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.39\n",
      "Sample Input: o in ecolo\n",
      "Sample Pred : oloce ni e\n",
      "Average loss at step 24200: 0.346741 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.53\n",
      "Sample Input: of the eth\n",
      "Sample Pred : hte eht fo\n",
      "Average loss at step 24400: 0.339272 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.42\n",
      "Sample Input: argued tha\n",
      "Sample Pred : aht deudre\n",
      "Average loss at step 24600: 0.337894 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.35\n",
      "Sample Input: sacrifice \n",
      "Sample Pred :  ecimircas\n",
      "Average loss at step 24800: 0.342900 learning rate: 0.001000\n",
      "Minibatch perplexity: 1.38\n",
      "Sample Input: rsonal enj\n",
      "Sample Pred : qne lenocs\n",
      "Average loss at step 25000: 0.345657 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.34\n",
      "Sample Input: as initial\n",
      "Sample Pred : laitini sa\n",
      "Average loss at step 25200: 0.333265 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.45\n",
      "Sample Input: science th\n",
      "Sample Pred : ht ecnehcs\n",
      "Average loss at step 25400: 0.336730 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.45\n",
      "Sample Input: also showe\n",
      "Sample Pred : ewohc otlo\n",
      "Average loss at step 25600: 0.338165 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.38\n",
      "Sample Input: ost always\n",
      "Sample Pred : syabla tso\n",
      "Average loss at step 25800: 0.339078 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.44\n",
      "Sample Input:  helping o\n",
      "Sample Pred : o gnimreh \n",
      "Average loss at step 26000: 0.349307 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.38\n",
      "Sample Input: moment it \n",
      "Sample Pred :  ti tnemom\n",
      "Average loss at step 26200: 0.343356 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.48\n",
      "Sample Input: ond editio\n",
      "Sample Pred : oitide dno\n",
      "Average loss at step 26400: 0.347441 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.43\n",
      "Sample Input: #ero #ero \n",
      "Sample Pred :  ore# ore#\n",
      "Average loss at step 26600: 0.342299 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.44\n",
      "Sample Input: nese two #\n",
      "Sample Pred : # owt esen\n",
      "Average loss at step 26800: 0.344462 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.43\n",
      "Sample Input: dre dumas \n",
      "Sample Pred :  samud erc\n",
      "Average loss at step 27000: 0.340984 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.40\n",
      "Sample Input: isit ameri\n",
      "Sample Pred : irema tisi\n",
      "Average loss at step 27200: 0.345082 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.34\n",
      "Sample Input: to mussoli\n",
      "Sample Pred : ilossuf ot\n",
      "Average loss at step 27400: 0.343738 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.44\n",
      "Sample Input: as the mor\n",
      "Sample Pred : rom eht sa\n",
      "Average loss at step 27600: 0.340839 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.47\n",
      "Sample Input: developed \n",
      "Sample Pred :  depoleves\n",
      "Average loss at step 27800: 0.338205 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.37\n",
      "Sample Input: etery valh\n",
      "Sample Pred : hlar yrete\n",
      "Average loss at step 28000: 0.343713 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.36\n",
      "Sample Input: she believ\n",
      "Sample Pred : veileb eht\n",
      "Average loss at step 28200: 0.339708 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.41\n",
      "Sample Input:  never be \n",
      "Sample Pred :  eb rever \n",
      "Average loss at step 28400: 0.341909 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.39\n",
      "Sample Input: documented\n",
      "Sample Pred : detnemucod\n",
      "Average loss at step 28600: 0.335703 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.45\n",
      "Sample Input:  s novels \n",
      "Sample Pred :  slevon s \n",
      "Average loss at step 28800: 0.341152 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.40\n",
      "Sample Input: ght to be \n",
      "Sample Pred :  eb ot tpg\n",
      "Average loss at step 29000: 0.344009 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.45\n",
      "Sample Input: rly ayn ra\n",
      "Sample Pred : ar nma yll\n",
      "Average loss at step 29200: 0.342570 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.36\n",
      "Sample Input: deliveranc\n",
      "Sample Pred : cnareviric\n",
      "Average loss at step 29400: 0.343348 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.41\n",
      "Sample Input: ers randex\n",
      "Sample Pred : xednar sne\n",
      "Average loss at step 29600: 0.343456 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.38\n",
      "Sample Input: ometry whi\n",
      "Sample Pred : ihw ylsemo\n",
      "Average loss at step 29800: 0.342456 learning rate: 0.000100\n",
      "Minibatch perplexity: 1.38\n",
      "Sample Input: anning ove\n",
      "Sample Pred : evo gninna\n",
      "Average loss at step 30000: 0.346986 learning rate: 0.000010\n",
      "Minibatch perplexity: 1.36\n",
      "Sample Input:  nine five\n",
      "Sample Pred : evif enin \n",
      "10\n",
      "Validation set perplexity: 1.39\n"
     ]
    }
   ],
   "source": [
    "num_steps = 30001\n",
    "summary_frequency = 200\n",
    "\n",
    "mir_train_batches.reset()\n",
    "mir_valid_batches.reset()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    \n",
    "    train_unrolling = list()\n",
    "    for step in range(num_steps):    \n",
    "        # Set up fetches dictionary\n",
    "        train_input_unrolling = mir_train_batches.next()\n",
    "        train_label_unrolling = train_input_unrolling[:]\n",
    "        train_label_unrolling.reverse()\n",
    "        \n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = train_input_unrolling[i]\n",
    "            feed_dict[train_labels[i]] = train_label_unrolling[i]\n",
    "            \n",
    "        # Run the session    \n",
    "        output_requested = [optimizer, \n",
    "                            loss, \n",
    "                            train_prediction, \n",
    "                            learning_rate]\n",
    "        output = session.run(output_requested, feed_dict=feed_dict)\n",
    "        output_loss = output[1]\n",
    "        predictions = output[2]\n",
    "        lr = output[3]\n",
    "        \n",
    "        mean_loss += output_loss\n",
    "        \n",
    "        # Log status\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            \n",
    "            # Log perplexity\n",
    "            labels = np.concatenate(train_label_unrolling)\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            \n",
    "            # Log the prediction for the first row of the unrolling\n",
    "            reshaped_unrolling = np.array(train_input_unrolling)[:, :1, :]\n",
    "            print('Sample Input:', mir_unrolling_to_strings(reshaped_unrolling)[0])\n",
    "            reshaped_pred = predictions.reshape((num_unrollings, batch_size, mir_vocabulary_size))\n",
    "            reshaped_pred = reshaped_pred[:, :1, :]\n",
    "            print('Sample Pred :', mir_unrolling_to_strings(reshaped_pred)[0])\n",
    "        \n",
    "        \n",
    "    # Measure validation set perplexity.\n",
    "    valid_logprob = 0\n",
    "    num_steps = mir_valid_size // (num_unrollings * batch_size)\n",
    "    for _ in range(num_steps):        \n",
    "        valid_unrolling = list()\n",
    "        \n",
    "        # Set up fetches dictionary\n",
    "        valid_input_unrolling = mir_valid_batches.next()\n",
    "        valid_label_unrolling = valid_input_unrolling[:]\n",
    "        valid_label_unrolling.reverse()\n",
    "\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = valid_input_unrolling[i]\n",
    "            feed_dict[train_labels[i]] = valid_label_unrolling[i]\n",
    "\n",
    "        # Run the session    \n",
    "        output_requested = [train_prediction]\n",
    "        output = session.run(output_requested, feed_dict=feed_dict)\n",
    "        valid_predictions = output[0]        \n",
    "        valid_labels = np.concatenate(valid_label_unrolling)\n",
    "        valid_logprob = valid_logprob + logprob(valid_predictions, valid_labels)\n",
    "        \n",
    "    print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / num_steps)))\n",
    "\n",
    "    mir_saver = tf.train.Saver([s1_x_weights, \n",
    "                                s1_x_bias, \n",
    "                                s1_mem_weights, \n",
    "                                s1_mem_bias, \n",
    "                                s2_x_weights, \n",
    "                                s2_x_bias, \n",
    "                                s2_mem_weights, \n",
    "                                s2_mem_bias, \n",
    "                                weights,\n",
    "                                biases])\n",
    "    \n",
    "    mir_saver.save(session, os.path.expanduser('~/Desktop/Mirror Model/mir_data'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I didn't vary the length of the training sequences, and I didn't use an 'eos' character to mark the end of input, I was a little concerned that the final network might not be able to handle sequences of arbitrary lengths. For example, I could imagine a situation where the encoder learned to wait until it saw the 10th character before formating its output in a way that would be usable by the decoder.\n",
    "\n",
    "That doesn't appear to have happened (see next cell). It looks as though the encoder learned to continuously encode an ordered sequence of 10 letters into a finite vector of size `num_nodes`, which could be understood by the decoder at any point in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: abcdf\n",
      "Pred: fdcba\n",
      "Input: cow\n",
      "Pred: woc\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:    \n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    mir_saver = tf.train.Saver([s1_x_weights, \n",
    "                                s1_x_bias, \n",
    "                                s1_mem_weights, \n",
    "                                s1_mem_bias, \n",
    "                                s2_x_weights, \n",
    "                                s2_x_bias, \n",
    "                                s2_mem_weights, \n",
    "                                s2_mem_bias, \n",
    "                                weights,\n",
    "                                biases])\n",
    "    mir_saver.restore(session, os.path.expanduser('~/Desktop/Mirror Model/mir_data'))\n",
    "    \n",
    "    def mirror(test_string):\n",
    "        print('Input:', test_string)\n",
    "        s1_reset_state.run()\n",
    "        s2_reset_state.run()\n",
    "        for char in test_string:\n",
    "            outputs = session.run([s1_store_state], {s1_sample_input: [mir_char_to_one_hot(char)]})\n",
    "        setup_s2.eval()\n",
    "        sentence = list()\n",
    "        for char in test_string:\n",
    "            sentence.append(sample_prediction.eval())\n",
    "        print('Pred:', mir_unrolling_to_strings(sentence)[0])\n",
    "        \n",
    "    test_string = 'ons anarch'\n",
    "    test_string = 'ons'\n",
    "    \n",
    "    mirror('abcdf')\n",
    "    mirror('cow')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
